import pandas as pd
import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import unicodedata
import string
from bot.handlers.utils import normalize_text, transliterate_abbreviation
import unicodedata
import string
from fuzzywuzzy import process, fuzz
import json
import os
from bot.handlers.query_processing.vet_abbreviations_expander import vet_abbr_manager

def load_disease_dictionary(excel_file_path: str) -> Tuple[Dict[str, str], Dict[str, List[str]]]:
    try:
        df = pd.read_excel(excel_file_path, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π')
        
        colloquial_to_official = {}
        abbr_to_official = {}
        
        # –î–û–ë–ê–í–õ–ï–ù–û: –û–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π -> –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        reverse_abbr_dict = defaultdict(list)
        
        for _, row in df.iterrows():
            official_name = normalize_text(row['–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º'])
            if not official_name:
                continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            colloquial_to_official[official_name] = official_name
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –†–ê–ó–ì–û–í–û–†–ù–´–• –ù–ê–ó–í–ê–ù–ò–ô - –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            colloquial_text = normalize_text(row['–†–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ'])
            if colloquial_text:
                for term in colloquial_text.split(','):
                    term = term.strip()
                    if term:
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ä–º–∏–Ω –∫–∞–∫ –µ—Å—Ç—å
                        colloquial_to_official[term] = official_name
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏–∑ —Ç–µ—Ä–º–∏–Ω–∞
                        potential_abbrs = extract_abbreviations_from_text(term)
                        for abbr in potential_abbrs:
                            if len(abbr) > 2:
                                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±–∞ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                                abbr_to_official[abbr] = official_name
                                colloquial_to_official[abbr] = official_name
                                colloquial_to_official[abbr.lower()] = official_name
                                # –î–û–ë–ê–í–õ–ï–ù–û: –í –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
                                if abbr not in reverse_abbr_dict[official_name]:
                                    reverse_abbr_dict[official_name].append(abbr)
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –ê–ë–ë–†–ï–í–ò–ê–¢–£–†
            abbr_text = str(row['–†–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞']).strip()
            if abbr_text:
                for abbr in abbr_text.split(','):
                    abbr_clean = abbr.strip().upper()
                    if abbr_clean:
                        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞
                        abbr_to_official[abbr_clean] = official_name
                        colloquial_to_official[abbr_clean] = official_name
                        colloquial_to_official[abbr_clean.lower()] = official_name
                        # –î–û–ë–ê–í–õ–ï–ù–û: –í –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
                        if abbr_clean not in reverse_abbr_dict[official_name]:
                            reverse_abbr_dict[official_name].append(abbr_clean)
                        
                        # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ä–∞—Å–∫–ª–∞–¥–æ–∫
                        mixed_variants = detect_and_normalize_mixed_abbreviations(abbr_clean)
                        for variant in mixed_variants:
                            if len(variant) >= 2:
                                abbr_to_official[variant] = official_name
                                colloquial_to_official[variant] = official_name
                                colloquial_to_official[variant.lower()] = official_name
        
        # –î–û–ë–ê–í–õ–ï–ù–û: –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ –õ–∏—Å—Ç2
        try:
            df_samples = pd.read_excel(excel_file_path, sheet_name='–õ–∏—Å—Ç2')
            for _, row in df_samples.iterrows():
                if len(df_samples.columns) >= 2:
                    abbr = str(row.iloc[0]).strip() if pd.notna(row.iloc[0]) else ""
                    full_name = str(row.iloc[1]).strip() if pd.notna(row.iloc[1]) else ""
                    
                    if abbr and abbr != 'nan' and full_name and full_name != 'nan':
                        abbr_upper = abbr.upper()
                        full_name_lower = full_name.lower()
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
                        if abbr_upper not in reverse_abbr_dict[full_name_lower]:
                            reverse_abbr_dict[full_name_lower].append(abbr_upper)
                        
                        # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                        colloquial_to_official[full_name_lower] = full_name_lower
                        colloquial_to_official[abbr_upper] = full_name_lower
                        abbr_to_official[abbr_upper] = full_name_lower
                        
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –æ–±—Ä–∞–∑—Ü–æ–≤: {e}")
        
        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ –∏–∑ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
        common_russian_words = {
            '–û–¢', '–î–û', '–ü–û', '–ù–ê', '–ó–ê', '–ò–ó', '–°', '–£', '–í', '–ö', '–ù–û', '–î–ê',
            '–ù–ï–¢', '–ê–ì–ê', '–û–ô', '–ê–•', '–≠–•', '–ù–£', '–í–û–¢', '–≠–¢–û', '–¢–û', '–¢–ê–ö', '–ö–ê–ö'
        }
        
        for word in common_russian_words:
            if word in abbr_to_official:
                del abbr_to_official[word]
            if word in colloquial_to_official and len(word) <= 3:
                # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ –∏ –Ω–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                if colloquial_to_official[word] != word:
                    del colloquial_to_official[word]

        # –î–û–ë–ê–í–õ–ï–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –≤ JSON
        with open('data/reverse_abbreviations.json', 'w', encoding='utf-8') as f:
            reverse_dict_serializable = {k: v for k, v in reverse_abbr_dict.items()}
            json.dump(reverse_dict_serializable, f, ensure_ascii=False, indent=2)

        return colloquial_to_official, abbr_to_official
        
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è: {e}")

def expand_query_with_abbreviations(query: str) -> str:
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–®–ò–†–ï–ù–ò–Ø –ó–ê–ü–†–û–°–û–í –° –û–ë–†–ê–¢–ù–´–ú –°–õ–û–í–ê–†–ï–ú"""
    
    # 1. –ü–ï–†–í–´–ô –≠–¢–ê–ü: –†–ê–°–®–ò–†–ï–ù–ò–ï –í–ï–¢–ï–†–ò–ù–ê–†–ù–´–ú–ò –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–ê–ú–ò
    print(f"üì• –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
    query = vet_abbr_manager.expand_query(query)
    print(f"üì§ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏: '{query}'")
    
    # 2. –í–¢–û–†–û–ô –≠–¢–ê–ü: –°–£–©–ï–°–¢–í–£–Æ–©–ê–Ø –õ–û–ì–ò–ö–ê –° –ë–û–õ–ï–ó–ù–Ø–ú–ò
    try:
        excel_file_path = 'data/processed/data_with_abbreviations_new.xlsx'
        colloquial_to_official, abbr_to_official = load_disease_dictionary(excel_file_path)
        
        tokens = advanced_query_tokenization(query)
        matched_officials = find_matches_with_context(tokens, colloquial_to_official, abbr_to_official, query)
        resolved_officials = handle_ambiguity(matched_officials, query, colloquial_to_official)
        
        if resolved_officials:
            sorted_officials = sorted(list(resolved_officials))
            expanded = f"{query} {' '.join(sorted_officials)}"
            
            # –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            expanded_with_reverse = apply_reverse_expansion(expanded)
            
            # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É —Å post_process_results
            final_result = post_process_results(expanded_with_reverse, query)
            print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{final_result}'")
            return final_result
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –±–æ–ª–µ–∑–Ω–µ–π: {e}")
    
    # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É –≤–æ–∑–≤—Ä–∞—Ç–∞
    final_result = post_process_results(query, query)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{final_result}'")
    return final_result



def generate_common_typos(abbr: str, is_russian: bool) -> List[str]:
    if len(abbr) <= 1:
        return []
    
    typos = set()
    
    # –û–ø–µ—á–∞—Ç–∫–∏ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –±—É–∫–≤
    if is_russian:
    # –û–ø–µ—á–∞—Ç–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤
        common_mistakes = {
            '–ê': ['–°', '–î', '–§', '–õ'],
            '–ë': ['–í', '–¨', '–´', '–™'],
            '–í': ['–ë', '–¨', '–´', '–§'],
            '–ì': ['–¢', '–ü', '–†'],
            '–î': ['–¢', '–õ', '–ñ'],
            '–ï': ['–≠', '–Å', '–ó'],
            '–ñ': ['–•', '–ö', '–î'],
            '–ó': ['–≠', '–ï', '–°'],
            '–ò': ['–ô', '–¶', '–£'],
            '–ô': ['–ò', '–¶', '–£'],
            '–ö': ['–ñ', '–•', '–ù'],
            '–õ': ['–î', '–ü', '–ú'],
            '–ú': ['–ù', '–õ', '–¢'],
            '–ù': ['–ú', '–ö', '–ü'],
            '–û': ['–ê', '–°', '–≠'],
            '–ü': ['–†', '–ù', '–õ'],
            '–†': ['–ü', '–ì', '–¨'],
            '–°': ['–ó', '–≠', '–û'],
            '–¢': ['–ì', '–ü', '–ú'],
            '–£': ['–ò', '–¶', '–ô'],
            '–§': ['–ê', '–í', '–•'],
            '–•': ['–ñ', '–§', '–ö'],
            '–¶': ['–£', '–ò', '–ô'],
            '–ß': ['–©', '–¨', '–´'],
            '–®': ['–©', '–ß', '–¨'],
            '–©': ['–®', '–ß', '–¨'],
            '–¨': ['–ë', '–í', '–´'],
            '–´': ['–¨', '–™', '–ë'],
            '–™': ['–¨', '–´', '–ë'],
            '–≠': ['–ï', '–ó', '–°'],
            '–Æ': ['–£', '–ò', '–ô'],
            '–Ø': ['–ê', '–£', '–ò']
        }
        
    else:
        # –û–ø–µ—á–∞—Ç–∫–∏ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –±—É–∫–≤ (–∫–∞–∫ —Ä–∞–Ω–µ–µ)
        common_mistakes = {
            'D': ['T', 'F', 'G', 'B'],
            'B': ['V', 'P', 'R', 'D'],
            'P': ['B', 'R', 'D'],
            'T': ['D', 'F', 'G'],
            'F': ['D', 'T', 'V'],
            'V': ['B', 'F', 'W'],
            'W': ['V', 'M', 'N'],
            'M': ['N', 'W'],
            'N': ['M', 'H'],
            'H': ['N', 'K'],
            'K': ['H', 'C'],
            'C': ['K', 'S'],
            'S': ['C', 'Z'],
            'Z': ['S', 'X'],
            'X': ['Z', 'K'],
            'G': ['D', 'T', 'J'],
            'J': ['G', 'I'],
            'I': ['J', 'L', '1'],
            'L': ['I', '1', '7'],
            '1': ['I', 'L', '7'],
            '7': ['1', 'L']
        }
    
    #–ó–∞–º–µ–Ω–∞ –æ–¥–Ω–æ–π –±—É–∫–≤—ã
    for i in range(len(abbr)):
        original_char = abbr[i]
        if original_char in common_mistakes:
            for mistake in common_mistakes[original_char]:
                typo = abbr[:i] + mistake + abbr[i+1:]
                typos.add(typo)
    #–ü—Ä–æ–ø—É—Å–∫ –±—É–∫–≤—ã
    for i in range(len(abbr)):
        typo = abbr[:i] + abbr[i+1:]
        if len(typo) >= 2:
            typos.add(typo)

    #–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –±—É–∫–≤—ã (–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ)
    for i in range(len(abbr)):
        typo = abbr[:i] + abbr[i] + abbr[i:]
        typos.add(typo)
    
    #–ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–æ—Å–µ–¥–Ω–∏—Ö –±—É–∫–≤
    for i in range(len(abbr)-1):
        typo = abbr[:i] + abbr[i+1] + abbr[i] + abbr[i+2:]
        typos.add(typo)
    
     #–ü—É—Ç–∞–Ω–∏—Ü–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞/–ª–∞—Ç–∏–Ω–∏—Ü–∞
    if is_russian:
        rus_lat_confusion = {
            '–ê': 'A', '–í': 'B', '–°': 'C', '–ï': 'E', '–ù': 'H', 
            '–ö': 'K', '–ú': 'M', '–û': 'O', '–†': 'P', '–¢': 'T',
            '–•': 'X', '–£': 'Y'
        }
        for i in range(len(abbr)):
            char = abbr[i]
            if char in rus_lat_confusion:
                typo = abbr[:i] + rus_lat_confusion[char] + abbr[i+1:]
                typos.add(typo)
    
    return list(typos)

def advanced_query_tokenization(query: str) -> List[Tuple[str, int, int]]:
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏–π
    query = normalize_text(query)
    tokens = []
    
    words = query.split()
    n = len(words)
    
    for start in range(n):
        for length in range(min(6, n - start), 0, -1):
            phrase = ' '.join(words[start:start+length])
            tokens.append((phrase, start, start+length))
    
    tokens.sort(key=lambda x: len(x[0]), reverse=True)
    return tokens


def find_matches_with_context(tokens: List[Tuple[str, int, int]], 
                             colloquial_to_official: Dict[str, str],
                             abbr_to_official: Dict[str, str],
                             query: str) -> Set[str]:
    
    matched_officials = set()
    used_positions = set()
    
    for token, start, end in tokens:
        if any(pos in used_positions for pos in range(start, end)):
            continue
        
        token_lower = token.lower()
        token_upper = token.upper()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
        if len(token_upper) < 2:
            continue
            
        # 1. –ò—â–µ–º –≤ —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏—è—Ö (–≤—Å–µ —Ä–µ–≥–∏—Å—Ç—Ä—ã)
        if token_lower in colloquial_to_official:
            official = colloquial_to_official[token_lower]
            matched_officials.add(official)
            used_positions.update(range(start, end))
            continue
            
        if token_upper in colloquial_to_official:
            official = colloquial_to_official[token_upper]
            matched_officials.add(official)
            used_positions.update(range(start, end))
            continue
        
        # 2. –ò—â–µ–º –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞—Ö
        if token_upper in abbr_to_official:
            official = abbr_to_official[token_upper]
            matched_officials.add(official)
            used_positions.update(range(start, end))
            continue
        
        # 3. –î–ª—è —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, —Å–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        if (2 <= len(token_upper) <= 4 and token_upper.isalpha() and
            (any(c.isascii() for c in token_upper) or 
             any(c in '–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø' for c in token_upper))):
            
            # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            possible_variants = detect_and_normalize_mixed_abbreviations(token_upper)
            
            for variant in possible_variants:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ–±–æ–∏—Ö —Å–ª–æ–≤–∞—Ä—è—Ö
                if variant in abbr_to_official:
                    official = abbr_to_official[variant]
                    matched_officials.add(official)
                    used_positions.update(range(start, end))
                    break
                    
                if variant in colloquial_to_official:
                    official = colloquial_to_official[variant]
                    matched_officials.add(official)
                    used_positions.update(range(start, end))
                    break
    
    return matched_officials


def handle_ambiguity(matched_officials: Set[str], query: str, colloquial_to_official: Dict[str, str]) -> Set[str]:
    if len(matched_officials) <= 1:
        return matched_officials
    
    query_lower = query.lower()
    query_words = query_lower.split()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
    has_abbreviations = any(len(word) <= 3 and word.isupper() for word in query_words)
    is_short_query = len(query_words) <= 2
    
    # –î–õ–Ø –ê–ë–ë–†–ï–í–ò–ê–¢–£–†: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –í–°–ï –ø–æ—Ö–æ–∂–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    if has_abbreviations or is_short_query:
        return matched_officials
    
    disease_scores = {}
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π mapping
    disease_to_terms = defaultdict(set)
    for term, official in colloquial_to_official.items():
        if official in matched_officials:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –±–æ–ª–µ–∑–Ω–∏
            if ',' in term:
                for sub_term in term.split(','):
                    clean_term = sub_term.strip().lower()
                    if clean_term:
                        disease_to_terms[official].add(clean_term)
            else:
                clean_term = term.strip().lower()
                if clean_term:
                    disease_to_terms[official].add(clean_term)
    
    # –î–ª—è –∫–∞–∂–¥–æ–π –±–æ–ª–µ–∑–Ω–∏ —Å—á–∏—Ç–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π score
    for disease, terms in disease_to_terms.items():
        total_score = 0
        matches_count = 0
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –±–æ–ª–µ–∑–Ω–∏ –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        all_terms_text = ' '.join(terms)
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å–æ –≤—Å–µ–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –±–æ–ª–µ–∑–Ω–∏
        overall_similarity = fuzz.token_set_ratio(query_lower, all_terms_text)
        total_score += overall_similarity * 0.6  # 60% –≤–µ—Å–∞
        
        # 2. –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤
        for word in query_words:
            if len(word) <= 2:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                continue
                
            word_found = False
            for term in terms:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ—Ä–º–∏–Ω
                if word in term:
                    total_score += 30  # –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
                    word_found = True
                    matches_count += 1
                    break
            
            if not word_found:
                # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                for term in terms:
                    if fuzz.partial_ratio(word, term) > 80:
                        total_score += 15  # –ú–µ–Ω—å—à–∏–π –±–æ–Ω—É—Å –∑–∞ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                        matches_count += 1
                        break
        
        # 3. –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if matches_count > 0:
            total_score += (matches_count / len(query_words)) * 100
        
        disease_scores[disease] = total_score
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
    sorted_diseases = sorted(disease_scores.items(), key=lambda x: x[1], reverse=True)
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –±–æ–ª–µ–∑–Ω–∏ —Å score > 150 –∏–ª–∏ —Ç–æ–ø-3
    best_diseases = set()
    for disease, score in sorted_diseases[:3]:
        if score > 160:
            best_diseases.add(disease)
    
    return best_diseases if best_diseases else matched_officials


def expand_query_with_abbreviations(query: str) -> str:
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –†–ê–°–®–ò–†–ï–ù–ò–Ø –ó–ê–ü–†–û–°–û–í –° –û–ë–†–ê–¢–ù–´–ú –°–õ–û–í–ê–†–ï–ú"""
    
    # 1. –ü–ï–†–í–´–ô –≠–¢–ê–ü: –†–ê–°–®–ò–†–ï–ù–ò–ï –í–ï–¢–ï–†–ò–ù–ê–†–ù–´–ú–ò –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–ê–ú–ò
    print(f"üì• –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
    query = vet_abbr_manager.expand_query(query)
    print(f"üì§ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏: '{query}'")
    
    # 2. –í–¢–û–†–û–ô –≠–¢–ê–ü: –°–£–©–ï–°–¢–í–£–Æ–©–ê–Ø –õ–û–ì–ò–ö–ê –° –ë–û–õ–ï–ó–ù–Ø–ú–ò
    try:
        excel_file_path = 'data/processed/data_with_abbreviations_new.xlsx'
        colloquial_to_official, abbr_to_official = load_disease_dictionary(excel_file_path)
        
        tokens = advanced_query_tokenization(query)
        matched_officials = find_matches_with_context(tokens, colloquial_to_official, abbr_to_official, query)
        resolved_officials = handle_ambiguity(matched_officials, query, colloquial_to_official)
        
        if resolved_officials:
            sorted_officials = sorted(list(resolved_officials))
            expanded = f"{query} {' '.join(sorted_officials)}"
            
            # –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            expanded_with_reverse = apply_reverse_expansion(expanded)
            
            # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É —Å post_process_results
            final_result = post_process_results(expanded_with_reverse, query)
            print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{final_result}'")
            return final_result
            
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–∏ –±–æ–ª–µ–∑–Ω–µ–π: {e}")
    
    # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ª–æ–≥–∏–∫—É –≤–æ–∑–≤—Ä–∞—Ç–∞
    final_result = post_process_results(query, query)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{final_result}'")
    return final_result


# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
def post_process_results(expanded_query: str, original_query: str) -> str:
    words = expanded_query.split()
    seen = set()
    result_words = []
    
    for word in words:
        if word not in seen:
            result_words.append(word)
            seen.add(word)
    
    result = ' '.join(result_words)
       
    if normalize_text(result) == normalize_text(original_query):
        return original_query
    
    return result


def apply_reverse_expansion(query: str) -> str:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∫ –ø–æ–ª–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º –≤ –∑–∞–ø—Ä–æ—Å–µ"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
        with open('data/reverse_abbreviations.json', 'r', encoding='utf-8') as f:
            reverse_abbr_dict = json.load(f)
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å: {e}")
        return query
    
    if not reverse_abbr_dict:
        return query
    
    query_lower = query.lower()
    words = query.split()
    expanded_terms = set(words)
    
    # –ò—â–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    for full_name, abbr_list in reverse_abbr_dict.items():
        if re.search(r'\b' + re.escape(full_name) + r'\b', query_lower) and abbr_list:
            for abbr in abbr_list:
                if abbr not in expanded_terms:
                    print(f"  üîÑ –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{full_name}' -> '{abbr}'")
                    expanded_terms.add(abbr)
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    for word in words:
        word_lower = word.lower()
        if word_lower in reverse_abbr_dict and len(word) > 2 and not word.isupper():
            for abbr in reverse_abbr_dict[word_lower]:
                if abbr not in expanded_terms:
                    print(f"  üîÑ –°–ª–æ–≤–æ -> –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞: '{word}' -> '{abbr}'")
                    expanded_terms.add(abbr)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    seen_terms = set()
    result_words = []
    
    # –°–Ω–∞—á–∞–ª–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    for word in words:
        word_lower = word.lower()
        if word_lower not in seen_terms:
            result_words.append(word)
            seen_terms.add(word_lower)
    
    # –ó–∞—Ç–µ–º –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    for term in expanded_terms:
        term_lower = term.lower()
        if term_lower not in seen_terms:
            result_words.append(term)
            seen_terms.add(term_lower)
    
    result = ' '.join(result_words)
    return result


def detect_and_normalize_mixed_abbreviations(text: str) -> List[str]:

    variants = set()
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤
    if len(text) < 2 or not any(c.isalpha() for c in text):
        return [text.upper()]
    
    text_upper = text.upper()
    variants.add(text_upper)
    
    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
    eng_to_rus = {
        'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì',
        'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù',
        'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£',
        'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
    }
    
    rus_to_eng = {
        '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
        '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
        '–ù': 'H', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
        '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
        '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
    }
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –±—É–∫–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    has_english = any(c.isascii() and c.isalpha() for c in text_upper)
    has_russian = any(c in '–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø' for c in text_upper)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±–µ —Ä–∞—Å–∫–ª–∞–¥–∫–∏ - —Å–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
    if has_english and has_russian:
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –≤—Å–µ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
        eng_variant = []
        for char in text_upper:
            if char in rus_to_eng:
                eng_variant.append(rus_to_eng[char])
            else:
                eng_variant.append(char)
        eng_result = ''.join(eng_variant)
        if eng_result and len(eng_result) >= 2:
            variants.add(eng_result)
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –≤—Å–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        rus_variant = []
        for char in text_upper:
            if char in eng_to_rus:
                rus_variant.append(eng_to_rus[char])
            else:
                rus_variant.append(char)
        rus_result = ''.join(rus_variant)
        if rus_result and len(rus_result) >= 2:
            variants.add(rus_result)
    
    # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —á–∏—Å—Ç–æ —Ä—É—Å—Å–∫–∏—Ö –∏–ª–∏ —á–∏—Å—Ç–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
    elif has_russian:
        # –†—É—Å—Å–∫–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ -> –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
        eng_variant = []
        for char in text_upper:
            if char in rus_to_eng:
                eng_variant.append(rus_to_eng[char])
            else:
                eng_variant.append(char)
        eng_result = ''.join(eng_variant)
        if eng_result and eng_result != text_upper and len(eng_result) >= 2:
            variants.add(eng_result)
    
    elif has_english:
        # –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ -> —Ä—É—Å—Å–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
        rus_variant = []
        for char in text_upper:
            if char in eng_to_rus:
                rus_variant.append(eng_to_rus[char])
            else:
                rus_variant.append(char)
        rus_result = ''.join(rus_variant)
        if rus_result and rus_result != text_upper and len(rus_result) >= 2:
            variants.add(rus_result)
    
    return list(variants)


def extract_abbreviations_from_text(text: str) -> List[str]:

    abbreviations = set()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞
    words = re.findall(r'\b[\w–ê-–Ø–∞-—è]+\b', text)
    
    for word in words:
        word_upper = word.upper()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä:

        if 2 < len(word_upper) <= 4 and word_upper.isalpha():
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            abbreviations.add(word_upper)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
            mixed_variants = detect_and_normalize_mixed_abbreviations(word_upper)
            for variant in mixed_variants:
                if len(variant) > 2:
                    abbreviations.add(variant)
    
    return list(abbreviations)
