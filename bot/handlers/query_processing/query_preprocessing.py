import pandas as pd
import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import json
import os
import pymorphy3
from fuzzywuzzy import fuzz, process
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UnifiedAbbreviationExpander:
    def __init__(self, excel_file: str = 'data/processed/data_with_abbreviations_new.xlsx'):
        self.morph = pymorphy3.MorphAnalyzer()
        self.excel_file = excel_file
        self.all_dictionaries = self._load_all_dictionaries()
        self.processed_queries = {}  # –ö—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.fuzzy_threshold = 80  # –ü–æ—Ä–æ–≥ –¥–ª—è fuzzy matching
        
        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.MAX_QUERY_LENGTH = 1000
        self.MAX_CACHE_SIZE = 1000
        self.MAX_WORD_LENGTH = 50
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è fuzzy –ø–æ–∏—Å–∫–∞
        self._build_fuzzy_search_indexes()
        
    def _validate_query(self, query: str) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        if not isinstance(query, str):
            return False
        if len(query) > self.MAX_QUERY_LENGTH:
            logger.warning(f"Query too long: {len(query)} characters")
            return False
        if any(word for word in query.split() if len(word) > self.MAX_WORD_LENGTH):
            logger.warning("Query contains overly long words")
            return False
        return True
        
    def _build_fuzzy_search_indexes(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è fuzzy –ø–æ–∏—Å–∫–∞ —Å –æ—Ç–ª–∞–¥–∫–æ–π"""
        self.fuzzy_full_names = []
        self.fuzzy_abbreviations = []
        
        logger.info("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è fuzzy –ø–æ–∏—Å–∫–∞...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        for dict_name in ['vet_full', 'disease_full', 'pcr_full']:
            dictionary = self.all_dictionaries.get(dict_name, {})
            for key, data in dictionary.items():
                if key and len(key) >= 2:
                    self.fuzzy_full_names.append((key, dict_name))
                    
        # –û–¢–õ–ê–î–ö–ê: –ø–æ—Å–º–æ—Ç—Ä–∏–º —á—Ç–æ –µ—Å—Ç—å –≤ disease_abbr
        disease_abbr_count = len(self.all_dictionaries.get('disease_abbr', {}))
        logger.info(f"üìä –í—Å–µ–≥–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –±–æ–ª–µ–∑–Ω–µ–π: {disease_abbr_count}")
        
        # –í—ã–≤–µ–¥–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –±–æ–ª–µ–∑–Ω–µ–π
        disease_examples = list(self.all_dictionaries.get('disease_abbr', {}).keys())[:5]
        logger.info(f"üìã –ü—Ä–∏–º–µ—Ä—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –±–æ–ª–µ–∑–Ω–µ–π: {disease_examples}")
        
        for dict_name in ['vet_abbr', 'disease_abbr', 'pcr_abbr']:
            dictionary = self.all_dictionaries.get(dict_name, {})
            dict_count = len(dictionary)
            logger.info(f"üìä –°–ª–æ–≤–∞—Ä—å {dict_name}: {dict_count} –∑–∞–ø–∏—Å–µ–π")
            
            for key, data in dictionary.items():
                if key and len(key) >= 2:
                    self.fuzzy_abbreviations.append((key, dict_name))
                    
                    # –û–¢–õ–ê–î–ö–ê: –¥–ª—è –í–ò–ö –∏ –î–¢–ë–°
                    if key in ['–í–ò–ö', '–î–¢–ë–°', 'VIK', 'DTBS']:
                        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞: {key} –≤ {dict_name}")
                        all_forms = data.get('all_abbr_forms', [])
                        logger.info(f"   –í—Å–µ —Ñ–æ—Ä–º—ã –¥–ª—è {key}: {all_forms}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
                    all_forms = data.get('all_abbr_forms', [])
                    for form in all_forms:
                        if form and len(form) >= 2:
                            self.fuzzy_abbreviations.append((form, dict_name))
                            
                            # –û–¢–õ–ê–î–ö–ê: –¥–ª—è —Ñ–æ—Ä–º –í–ò–ö
                            if key in ['–í–ò–ö', '–î–¢–ë–°'] and form in ['VIK', 'vik', 'DTBS', 'dtbs']:
                                logger.info(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ñ–æ—Ä–º–∞ {form} –¥–ª—è {key}")
        
        logger.info(f"üìà –í—Å–µ–≥–æ —Ñ–æ—Ä–º –¥–ª—è –ø–æ–∏—Å–∫–∞: {len(self.fuzzy_abbreviations)}")
        
        # –í—ã–≤–µ–¥–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –¥–ª—è –í–ò–ö –∏ –î–¢–ë–°
        vik_forms = [form for form, dict_name in self.fuzzy_abbreviations if '–í–ò–ö' in form or 'VIK' in form.upper()]
        dtbs_forms = [form for form, dict_name in self.fuzzy_abbreviations if '–î–¢–ë–°' in form or 'DTBS' in form.upper()]
        
        logger.info(f"üîç –§–æ—Ä–º—ã –¥–ª—è –í–ò–ö: {vik_forms}")
        logger.info(f"üîç –§–æ—Ä–º—ã –¥–ª—è –î–¢–ë–°: {dtbs_forms}")
    
    def _load_all_dictionaries(self) -> Dict[str, Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ Excel —Ñ–∞–π–ª–∞"""
        all_dicts = {
            'vet_abbr': {}, 'vet_full': {}, 
            'disease_abbr': {}, 'disease_full': {},
            'pcr_abbr': {}, 'pcr_full': {}
        }
        
        try:
            file_path = Path(self.excel_file)
            if not file_path.exists():
                logger.error(f"Excel file not found: {self.excel_file}")
                return all_dicts
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ Excel
            with pd.ExcelFile(self.excel_file, engine='openpyxl') as xls:
                # –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                if '–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∏–∏' in xls.sheet_names:
                    df_vet = pd.read_excel(xls, sheet_name='–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∏–∏')
                    self._process_vet_abbreviations(df_vet, all_dicts)
                
                # –ë–æ–ª–µ–∑–Ω–∏
                if '–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π' in xls.sheet_names:
                    df_disease = pd.read_excel(xls, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π')
                    self._process_diseases(df_disease, all_dicts)
                
                # –ü–¶–† —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
                if '–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†' in xls.sheet_names:
                    df_pcr = pd.read_excel(xls, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†')
                    self._process_pcr_abbreviations(df_pcr, all_dicts)
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å–ª–æ–≤–∞—Ä–µ–π: –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ({len(all_dicts['vet_abbr'])}), "
                       f"–ë–æ–ª–µ–∑–Ω–∏({len(all_dicts['disease_abbr'])}), –ü–¶–†({len(all_dicts['pcr_abbr'])})")
            
            # –û–¢–õ–ê–î–ö–ê: –ø—Ä–æ–≤–µ—Ä–∏–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            for abbr in ['–í–ò–ö', '–î–¢–ë–°', 'VIK', 'DTBS']:
                for dict_name in ['disease_abbr', 'vet_abbr']:
                    if abbr in all_dicts[dict_name]:
                        logger.info(f"üîç –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ '{abbr}' –Ω–∞–π–¥–µ–Ω–∞ –≤ {dict_name}")
                        data = all_dicts[dict_name][abbr]
                        logger.info(f"   –î–∞–Ω–Ω—ã–µ: {data.get('official_name', 'N/A')}")
                        logger.info(f"   –í—Å–µ —Ñ–æ—Ä–º—ã: {data.get('all_abbr_forms', [])}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä–µ–π: {e}")
            
        return all_dicts
    
    def _safe_string_conversion(self, value) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É"""
        if pd.isna(value) or value is None:
            return ""
        return str(value).strip()
    
    def _process_vet_abbreviations(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row['–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞'])
                russian_name = self._safe_string_conversion(row['–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'])
                english_name = self._safe_string_conversion(row.get('–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ', ''))
                
                if not abbr or abbr == 'nan' or not russian_name or russian_name == 'nan':
                    continue
                    
                abbr_variants = self._split_variants(abbr, '/')
                russian_variants = self._split_variants(russian_name, '/')
                
                for abbr_var in abbr_variants:
                    if not abbr_var:
                        continue
                        
                    all_abbr_forms = self._generate_all_abbreviation_forms(abbr_var)
                    
                    for abbr_form in all_abbr_forms:
                        all_dicts['vet_abbr'][abbr_form] = {
                            'type': 'vet_abbr',
                            'russian_names': russian_variants,
                            'english_name': english_name,
                            'original_abbr': abbr_var,
                            'all_abbr_forms': all_abbr_forms
                        }
                        
                    for rus_name in russian_variants:
                        if not rus_name:
                            continue
                            
                        all_rus_forms = self._generate_all_forms(rus_name, is_abbreviation=False)
                        for rus_form in all_rus_forms:
                            all_dicts['vet_full'][rus_form] = {
                                'type': 'vet_full',
                                'abbreviations': [abbr_var],  # –¢–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
                                'original_name': rus_name,
                                'original_abbr': abbr_var,
                                'all_abbr_forms': all_abbr_forms
                            }
                
                if english_name and english_name != 'nan':
                    eng_forms = self._generate_all_forms(english_name, is_abbreviation=False)
                    for eng_form in eng_forms:
                        all_dicts['vet_full'][eng_form] = {
                            'type': 'vet_english',
                            'abbreviations': [abbr_var],  # –¢–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
                            'original_name': english_name,
                            'original_abbr': abbr_var,
                            'all_abbr_forms': all_abbr_forms
                        }
                        
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä: {e}")
                continue

    def _generate_all_abbreviation_forms(self, abbr: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        if not abbr:
            return []
            
        forms = set()
        forms.add(abbr)
        forms.add(abbr.upper())
        forms.add(abbr.lower())
        forms.add(abbr.capitalize())
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        translit_variants = self._generate_transliteration_variants(abbr)
        forms.update(translit_variants)
        
        # –û–¢–õ–ê–î–ö–ê: –¥–ª—è –í–ò–ö –∏ –î–¢–ë–°
        if abbr in ['–í–ò–ö', '–î–¢–ë–°']:
            logger.info(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–æ—Ä–º –¥–ª—è '{abbr}': {list(forms)}")
        
        return [form for form in forms if form and 1 < len(form) <= 20]

    def _process_diseases(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–æ–ª–µ–∑–Ω–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
        logger.info("ü©∫ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª–µ–∑–Ω–µ–π...")
        
        for _, row in df.iterrows():
            try:
                official_name = self._safe_string_conversion(row['–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º'])
                colloquial_names = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ', ''))
                abbreviations = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                
                if not official_name or official_name == 'nan':
                    continue
                
                # –û–¢–õ–ê–î–ö–ê: –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –≤—ã–≤–µ–¥–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª–µ–∑–Ω–∏
                if '–∏–º–º—É–Ω–æ–¥–µ—Ñ–∏—Ü–∏—Ç' in official_name.lower() or '–¥–∏—Å–ø–ª–∞–∑–∏—è' in official_name.lower():
                    logger.info(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –±–æ–ª–µ–∑–Ω—å: {official_name}")
                    logger.info(f"   –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: {abbreviations}")
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
                official_forms = self._generate_medical_term_forms(official_name)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –±–æ–ª–µ–∑–Ω–µ–π (–≤–∫–ª—é—á–∞—è –î–¢–ë–°, –í–ò–ö –∏ –¥—Ä—É–≥–∏–µ)
                original_abbreviations = []
                all_abbr_forms_for_disease = []
                
                if abbreviations:
                    abbr_variants = self._split_variants(abbreviations, ',')
                    original_abbreviations = abbr_variants
                    
                    for abbr_var in abbr_variants:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–≤–∫–ª—é—á–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—é)
                        all_abbr_forms = self._generate_all_abbreviation_forms(abbr_var)
                        all_abbr_forms_for_disease.extend(all_abbr_forms)
                        
                        # –û–¢–õ–ê–î–ö–ê: –¥–ª—è –í–ò–ö –∏ –î–¢–ë–°
                        if abbr_var in ['–í–ò–ö', '–î–¢–ë–°']:
                            logger.info(f"üéØ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞: {abbr_var}")
                            logger.info(f"   –í—Å–µ —Ñ–æ—Ä–º—ã: {all_abbr_forms}")
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –≤ —Å–ª–æ–≤–∞—Ä—å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
                        for abbr_form in all_abbr_forms:
                            all_dicts['disease_abbr'][abbr_form] = {
                                'type': 'disease_abbr',
                                'official_name': official_name,
                                'original_abbr': abbr_var,
                                'all_abbr_forms': all_abbr_forms
                            }
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                for form in official_forms:
                    all_dicts['disease_full'][form] = {
                        'type': 'disease_full',
                        'official_name': official_name,
                        'abbreviations': original_abbreviations,  # –¢–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
                        'original_name': official_name,
                        'all_abbr_forms': all_abbr_forms_for_disease
                    }
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                if colloquial_names:
                    colloquial_variants = self._split_variants(colloquial_names, ',')
                    for colloquial_var in colloquial_variants:
                        colloquial_forms = self._generate_medical_term_forms(colloquial_var)
                        for form in colloquial_forms:
                            all_dicts['disease_full'][form] = {
                                'type': 'disease_colloquial',
                                'official_name': official_name,
                                'abbreviations': original_abbreviations,  # –¢–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
                                'original_name': colloquial_var,
                                'all_abbr_forms': all_abbr_forms_for_disease
                            }
                            
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –±–æ–ª–µ–∑–Ω–µ–π: {e}")
                continue

    def _generate_medical_term_forms(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        if not text:
            return []
            
        forms = set()
        forms.add(text)
        forms.add(text.lower())
        forms.add(text.upper())
        
        return list(forms)

    def _process_pcr_abbreviations(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ü–¶–† —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è"""
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row['–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞'])
                full_name = self._safe_string_conversion(row['–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'])
                
                if not abbr or abbr == 'nan' or not full_name or full_name == 'nan':
                    continue
                
                abbr_variants = self._split_variants(abbr, '/')
                full_name_variants = self._split_variants(full_name, '/')
                
                for abbr_var in abbr_variants:
                    all_abbr_forms = self._generate_all_abbreviation_forms(abbr_var)
                    for abbr_form in all_abbr_forms:
                        all_dicts['pcr_abbr'][abbr_form] = {
                            'type': 'pcr_abbr',
                            'full_names': full_name_variants,
                            'original_abbr': abbr_var,
                            'all_abbr_forms': all_abbr_forms
                        }
                
                for full_var in full_name_variants:
                    all_full_forms = self._generate_all_forms(full_var, is_abbreviation=False)
                    for full_form in all_full_forms:
                        all_dicts['pcr_full'][full_form] = {
                            'type': 'pcr_full',
                            'abbreviations': abbr_variants,  # –¢–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
                            'original_name': full_var,
                            'all_abbr_forms': [abbr_form for abbr_var in abbr_variants 
                                            for abbr_form in self._generate_all_abbreviation_forms(abbr_var)]
                        }
                        
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –ü–¶–†: {e}")
                continue
    
    def _split_variants(self, text: str, delimiter: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç—ã"""
        if not text:
            return []
        
        variants = [v.strip() for v in text.split(delimiter) if v.strip()]
        return variants if variants else [text]
    
    def _generate_all_forms(self, text: str, is_abbreviation: bool = False) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞/–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        if not text:
            return []
            
        if is_abbreviation:
            # –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
            return self._generate_all_abbreviation_forms(text)
        else:
            forms = set()
            forms.add(text)
            forms.add(text.lower())
            forms.add(text.upper())
            return list(forms)
    
    def _generate_transliteration_variants(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        variants = set()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
        eng_to_rus = {
            'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì',
            'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù',
            'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£',
            'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
        }
        
        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'H', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–™': '', '–´': 'Y', '–¨': '', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
        }
        
        text_upper = text.upper()
        
        # –†—É—Å—Å–∫–∏–π -> –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
        if any(c in '–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø' for c in text_upper):
            eng_variant = ''.join(rus_to_eng.get(char, char) for char in text_upper)
            if eng_variant and eng_variant != text_upper:
                variants.add(eng_variant)
                variants.add(eng_variant.lower())
                variants.add(eng_variant.capitalize())
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π -> —Ä—É—Å—Å–∫–∏–π  
        elif any(c.isascii() and c.isalpha() for c in text_upper):
            rus_variant = ''.join(eng_to_rus.get(char, char) for char in text_upper)
            if rus_variant and rus_variant != text_upper:
                variants.add(rus_variant)
                variants.add(rus_variant.lower())
                variants.add(rus_variant.capitalize())
        
        # –û–¢–õ–ê–î–ö–ê: –¥–ª—è –í–ò–ö –∏ –î–¢–ë–°
        if text in ['–í–ò–ö', '–î–¢–ë–°']:
            logger.info(f"üîÑ –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –¥–ª—è '{text}': {list(variants)}")
        
        return list(variants)
    
    def expand_query(self, query: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        if not self._validate_query(query):
            return query
            
        logger.info(f"üì• –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
        
        # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å
        query = ' '.join(query.split())
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
        if len(self.processed_queries) > self.MAX_CACHE_SIZE:
            self.processed_queries.clear()
            
        if query in self.processed_queries:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            return self.processed_queries[query]
        
        if self._has_proper_expansions(query):
            logger.info("‚úÖ –ó–∞–ø—Ä–æ—Å —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            self.processed_queries[query] = query
            return query
        
        corrected_query = self._fix_typos(query)
        if corrected_query != query:
            logger.info(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –æ–ø–µ—á–∞—Ç–∫–∏: '{query}' -> '{corrected_query}'")
            query = corrected_query
        
        matches = self._find_matches_with_fuzzy(query)
        result = self._apply_expansions(query, matches)
        
        self.processed_queries[query] = result
        
        if result != query:
            logger.info(f"üì§ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: '{result}'")
        else:
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{result}'")
        
        return result

    def _fix_typos(self, query: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏"""
        common_typos = {
            '—Ñ—Å–ø–µ—Ä–º–∞': '—Å–ø–µ—Ä–º–∞', '—Ñ—Å–ø–µ—Ä–º—ã': '—Å–ø–µ—Ä–º—ã', '—Ñ—Å–ø–µ—Ä–º—É': '—Å–ø–µ—Ä–º—É',
            '—Ñ—Å–ø–µ—Ä–º–æ–π': '—Å–ø–µ—Ä–º–æ–π', '–∫–∞–ª–ª': '–∫–∞–ª', '—Ñ–µ–∫–∞–ª–∏': '—Ñ–µ–∫–∞–ª–∏–∏',
            '—Ñ–µ–∫–∞–ª–∏–π': '—Ñ–µ–∫–∞–ª–∏–∏', '—Ñ–µ–∫–∞–ª–∏—è': '—Ñ–µ–∫–∞–ª–∏–∏', '–µ–∫—Å–∫—Ä–µ–º–µ–Ω—Ç—ã': '—ç–∫—Å–∫—Ä–µ–º–µ–Ω—Ç—ã',
            '–µ–∫—Å–∫—Ä–µ–º–µ–Ω—Ç–æ–≤': '—ç–∫—Å–∫—Ä–µ–º–µ–Ω—Ç–æ–≤', '–∏–∏': '–∏',
        }
        
        words = query.split()
        corrected_words = []
        
        for word in words:
            if (word.startswith('(') and word.endswith(')')) or self._is_abbreviation(word):
                corrected_words.append(word)
                continue
                
            if word.islower() or (word[0].isupper() and word[1:].islower()):
                corrected_word = common_typos.get(word.lower(), word)
                if word[0].isupper():
                    corrected_word = corrected_word.capitalize()
                corrected_words.append(corrected_word)
            else:
                corrected_words.append(word)
        
        return ' '.join(corrected_words)

    def _is_abbreviation(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π"""
        if word.isupper() and 2 <= len(word) <= 5:
            return True
        if word.isalpha() and word.isupper():
            return True
        if len(word) <= 4 and any(c.isupper() for c in word):
            return True
        return False
    
    def _has_proper_expansions(self, query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏"""
        cleaned_query = self._fix_typos(query)
        
        patterns = [
            r'[–∞-—è–ê-–Øa-zA-Z]+\s*\([^)]+\)',
            r'\([^)]+\)\s*[–∞-—è–ê-–Øa-zA-Z]+',
        ]
        
        for pattern in patterns:
            if re.search(pattern, cleaned_query):
                return True
        
        bracket_pattern = r'\(([^)]+)\)'
        brackets = re.findall(bracket_pattern, cleaned_query)
        
        for bracket_content in brackets:
            content = bracket_content.strip()
            if ',' in content:
                parts = [part.strip() for part in content.split(',')]
                if all(2 <= len(part) <= 5 and part.isupper() for part in parts):
                    return True
            elif 2 <= len(content) <= 30:
                return True
        
        words = re.findall(r'\b[–∞-—è–ê-–Øa-zA-Z]{2,}\b', cleaned_query)
        expansion_parts = re.findall(r'\([^)]+\)', cleaned_query)
        
        if len(expansion_parts) >= len(words):
            return True
        
        return False
    
    def _find_matches_with_fuzzy(self, query: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        matches = []
        used_positions = set()
        
        corrected_query = self._fix_typos(query)
        
        logger.info(f"üîç –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è: '{corrected_query}'")
        
        # –°–Ω–∞—á–∞–ª–∞ —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
        exact_matches = self._find_exact_matches(corrected_query, used_positions)
        matches.extend(exact_matches)
        
        # –ó–∞—Ç–µ–º fuzzy –ø–æ–∏—Å–∫ –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Å–ª–æ–≤
        fuzzy_matches = self._find_fuzzy_matches(corrected_query, used_positions)
        matches.extend(fuzzy_matches)
        
        logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        for match in matches:
            logger.info(f"   - '{match['found_text']}' -> {match['dict_name']}")
        
        return matches

    def _find_exact_matches(self, query: str, used_positions: set) -> List[Dict]:
        """–¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤"""
        matches = []
        
        words = query.split()
        
        for word in words:
            word_start = query.find(word)
            word_end = word_start + len(word)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ
            if len(word) < 2 or self._is_position_used(word_start, word_end, used_positions):
                continue
            
            # –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
            search_terms = [word]
            if word.isupper() and 2 <= len(word) <= 5:
                search_terms.append(word)  # –ò—â–µ–º –∫–∞–∫ –µ—Å—Ç—å
            else:
                search_terms.append(word.upper())  # –ò—â–µ–º –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
            
            # –ò—â–µ–º –≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ä—è—Ö
            for dict_name in ['vet_abbr', 'vet_full', 'disease_abbr', 'disease_full', 'pcr_abbr', 'pcr_full']:
                dictionary = self.all_dictionaries.get(dict_name, {})
                
                for search_term in search_terms:
                    if search_term in dictionary:
                        data = dictionary[search_term]
                        matches.append({
                            'start': word_start,
                            'end': word_end,
                            'found_text': word,
                            'dict_name': dict_name,
                            'data': data,
                            'match_type': 'exact'
                        })
                        used_positions.add((word_start, word_end))
                        logger.debug(f"–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: '{word}' -> {dict_name} (–ø–æ–∏—Å–∫: '{search_term}')")
                        break
                
                # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –≤ –æ–¥–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ
                if any(m['found_text'] == word for m in matches):
                    break
        
        return matches

    def _find_fuzzy_matches(self, query: str, used_positions: set) -> List[Dict]:
        """Fuzzy –ø–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
        matches = []
        words = query.split()
        
        for i, word in enumerate(words):
            word_start = query.find(word)
            word_end = word_start + len(word)
            
            if len(word) < 2 or self._is_position_used(word_start, word_end, used_positions):
                continue
            
            # –û–¢–õ–ê–î–ö–ê: –¥–ª—è VIK, DTBS
            if word.upper() in ['VIK', 'DTBS']:
                logger.info(f"üîç Fuzzy –ø–æ–∏—Å–∫ –¥–ª—è: '{word}'")
                logger.info(f"   –î–æ—Å—Ç—É–ø–Ω–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞: {len(self.fuzzy_abbreviations)}")
            
            # Fuzzy –ø–æ–∏—Å–∫ –ø–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º (–≤–∫–ª—é—á–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã)
            if len(self.fuzzy_abbreviations) > 0:
                try:
                    abbr_matches = process.extractBests(
                        word.upper(), 
                        [item[0] for item in self.fuzzy_abbreviations], 
                        scorer=fuzz.ratio, 
                        score_cutoff=self.fuzzy_threshold,
                        limit=3  # –£–≤–µ–ª–∏—á–∏–º –ª–∏–º–∏—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    )
                    
                    # –û–¢–õ–ê–î–ö–ê: –≤—ã–≤–µ–¥–µ–º —Ç–æ–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    if word.upper() in ['VIK', 'DTBS'] and abbr_matches:
                        logger.info(f"   –¢–æ–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è '{word}': {abbr_matches}")
                    
                    for match_text, score in abbr_matches:
                        for original_key, dict_name in self.fuzzy_abbreviations:
                            if original_key == match_text:
                                data = self.all_dictionaries[dict_name].get(original_key)
                                if data:
                                    matches.append({
                                        'start': word_start,
                                        'end': word_end,
                                        'found_text': word,
                                        'dict_name': dict_name,
                                        'data': data,
                                        'match_type': 'fuzzy',
                                        'score': score
                                    })
                                    used_positions.add((word_start, word_end))
                                    
                                    # –û–¢–õ–ê–î–ö–ê
                                    if word.upper() in ['VIK', 'DTBS']:
                                        logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: '{word}' -> '{match_text}' (score: {score})")
                                        logger.info(f"      –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {data.get('official_name', 'N/A')}")
                                    
                                break
                        break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ fuzzy –ø–æ–∏—Å–∫–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä: {e}")
        
        return matches

    def _is_position_used(self, start: int, end: int, used_positions: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è"""
        for used_start, used_end in used_positions:
            if not (end <= used_start or start >= used_end):
                return True
        return False

    def _apply_expansions(self, query: str, matches: List[Dict]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫ –∑–∞–ø—Ä–æ—Å—É"""
        if not matches:
            return query
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –ø–æ–∑–∏—Ü–∏–∏ (—Å –Ω–∞—á–∞–ª–∞ –∫ –∫–æ–Ω—Ü—É)
        matches.sort(key=lambda x: x['start'])
        
        result = query
        offset = 0  # –°–º–µ—â–µ–Ω–∏–µ –∏–∑-–∑–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∑–∞–º–µ–Ω
        
        for match in matches:
            start = match['start'] + offset
            end = match['end'] + offset
            found_text = match['found_text']
            dict_name = match['dict_name']
            data = match['data']
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ —á–∞—Å—Ç—å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ –≤ —Å–∫–æ–±–∫–∞—Ö
            if self._is_part_of_expansion(result, start, end):
                logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º '{found_text}' - —á–∞—Å—Ç—å —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏")
                continue
                
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            expanded_text = self._create_expanded_text(found_text, dict_name, data)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if expanded_text == found_text:
                continue
                
            # –ó–∞–º–µ–Ω—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            result_before = result
            result = result[:start] + expanded_text + result[end:]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
            offset += len(expanded_text) - len(found_text)
            
            logger.debug(f"–†–∞—Å—à–∏—Ä–µ–Ω–æ: '{found_text}' -> '{expanded_text}'")
            logger.debug(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: '{result}'")
        
        return result
        
    def _is_part_of_expansion(self, query: str, start: int, end: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        text_before = query[:start]
        open_brackets = text_before.count('(')
        close_brackets = text_before.count(')')
        return open_brackets > close_brackets

    def _create_expanded_text(self, found_text: str, dict_name: str, data: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –≤–Ω—É—Ç—Ä–∏ —Å–∫–æ–±–æ–∫
        if self._is_inside_brackets(found_text):
            return found_text
        
        # –ù–ï –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã! –û–Ω–∏ —Ç–æ–∂–µ –¥–æ–ª–∂–Ω—ã —Ä–∞—Å—à–∏—Ä—è—Ç—å—Å—è
        # if found_text.isupper() and len(found_text) <= 5:
        #     logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É: '{found_text}'")
        #     return found_text
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Å–ª–æ–≤–æ —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–∫–æ–±–∫–∏
        if '(' in found_text or ')' in found_text:
            return found_text
            
        expansion_rules = {
            'vet_abbr': self._expand_vet_abbr,
            'vet_full': self._expand_vet_full,
            'vet_english': self._expand_vet_english,
            'disease_abbr': self._expand_disease_abbr,
            'disease_full': self._expand_disease_full,
            'pcr_abbr': self._expand_pcr_abbr,
            'pcr_full': self._expand_pcr_full
        }
        
        expander = expansion_rules.get(dict_name)
        if expander:
            result = expander(found_text, data)
            if result != found_text:
                logger.debug(f"–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{found_text}' -> '{result}'")
                return result
        
        return found_text

    def _expand_vet_abbr(self, found_text: str, data: Dict) -> str:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        russian_names = data.get('russian_names', [])
        if russian_names:
            original_abbr = data.get('original_abbr', found_text.upper())
            # –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫—É
            return f"{original_abbr} ({russian_names[0]})"
        return found_text

    def _expand_vet_full(self, found_text: str, data: Dict) -> str:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–ª–Ω—ã–µ –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        abbreviations = data.get('abbreviations', [])
        if abbreviations:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞
            unique_abbrs = list(set(abbreviations))
            if unique_abbrs and not self._abbreviations_already_in_query(found_text, unique_abbrs):
                return f"{found_text} ({', '.join(unique_abbrs)})"
        return found_text

    def _expand_vet_english(self, found_text: str, data: Dict) -> str:
        return self._expand_vet_full(found_text, data)

    def _expand_disease_abbr(self, found_text: str, data: Dict) -> str:
        official_name = data.get('official_name', '')
        if official_name:
            original_abbr = data.get('original_abbr', found_text.upper())
            return f"{original_abbr} ({official_name})"
        return found_text

    def _expand_disease_full(self, found_text: str, data: Dict) -> str:
        abbreviations = data.get('abbreviations', [])
        official_name = data.get('official_name', '')
        
        is_colloquial = data.get('type') == 'disease_colloquial'
        is_official = data.get('type') == 'disease_full'
        
        if is_colloquial and official_name:
            return f"{found_text} ({official_name})"
        elif abbreviations:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞
            unique_abbrs = list(set(abbreviations))
            if unique_abbrs and not self._abbreviations_already_in_query(found_text, unique_abbrs):
                return f"{found_text} ({', '.join(unique_abbrs)})"
        elif official_name and official_name != found_text and is_official:
            return f"{found_text} ({official_name})"
        
        return found_text

    def _expand_pcr_abbr(self, found_text: str, data: Dict) -> str:
        full_names = data.get('full_names', [])
        if full_names:
            original_abbr = data.get('original_abbr', found_text.upper())
            return f"{original_abbr} ({full_names[0]})"
        return found_text

    def _expand_pcr_full(self, found_text: str, data: Dict) -> str:
        abbreviations = data.get('abbreviations', [])
        if abbreviations:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –¥–ª—è –≤—ã–≤–æ–¥–∞
            unique_abbrs = list(set(abbreviations))
            if unique_abbrs and not self._abbreviations_already_in_query(found_text, unique_abbrs):
                return f"{found_text} ({', '.join(unique_abbrs)})"
        return found_text

    def _is_inside_brackets(self, text: str) -> bool:
        return text.startswith('(') and text.endswith(')')

    def _abbreviations_already_in_query(self, found_text: str, abbreviations: List[str]) -> bool:
        return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
abbreviation_expander = UnifiedAbbreviationExpander()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def expand_query_with_abbreviations(query: str) -> str:
    return abbreviation_expander.expand_query(query)