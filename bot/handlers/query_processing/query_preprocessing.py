import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
from pathlib import Path
import logging
import pandas as pd
import pymorphy3
from fuzzywuzzy import fuzz, process

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class UnifiedAbbreviationExpander:
    def __init__(self, excel_file: str = 'data/processed/data_with_abbreviations_new.xlsx'):
        self.morph = pymorphy3.MorphAnalyzer()
        self.excel_file = excel_file

        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏
        self.all_dictionaries: Dict[str, Dict] = self._load_all_dictionaries()
        self.processed_queries: Dict[str, str] = {}

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.fuzzy_threshold = 75
        self.MAX_CACHE_SIZE = 5000
        self.min_word_length = 2

        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        self.ignore_words = {
            '–Ω–∞', '–≤', '—Å', '—É', '–æ', '–ø–æ', '–∑–∞', '–∏–∑', '–æ—Ç', '–¥–æ', '–¥–ª—è', '–ø—Ä–æ',
            '–∏', '–∏–ª–∏', '–Ω–æ', '–∞', '–¥–∞', '–ª–∏–±–æ', '–Ω–∏', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏',
            '—è', '—Ç—ã', '–≤—ã', '–º—ã', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–º–æ–π', '—Ç–≤–æ–π', '–≤–∞—à', '–Ω–∞—à',
            '–Ω–µ', '–ª–∏', '–±—ã', '–∂–µ', '–≤–æ—Ç', '–∫–∞–∫', '—Ç–∞–∫', '—Ç–∞–∫–∂–µ', '—Ç–æ–∂–µ',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be'
        }

        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        self.fuzzy_full_names: List[tuple] = []
        self.fuzzy_abbreviations: List[tuple] = []

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.match_stats = defaultdict(int)

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        self._build_fuzzy_search_indexes()

    # ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

    def _safe_string_conversion(self, value) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É."""
        if pd.isna(value) or value is None:
            return ""
        return str(value).strip()

    def _split_variants(self, text: str, delimiter: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é."""
        if not text:
            return []
        variants = [v.strip() for v in re.split(re.escape(delimiter), text) if v.strip()]
        return variants if variants else [text]

    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç - —É–±–∏—Ä–∞–µ—Ç –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã."""
        return ' '.join(text.split())

    # ==================== –û–ë–†–ê–ë–û–¢–ö–ê –û–ü–ï–ß–ê–¢–û–ö ====================

    def _fix_typos(self, query: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç —á–∞—Å—Ç—ã–µ –æ–ø–µ—á–∞—Ç–∫–∏ –≤ –∑–∞–ø—Ä–æ—Å–µ."""
        if not query:
            return query

        # –¢–∞–±–ª–∏—Ü–∞ –∑–∞–º–µ–Ω—ã —á–∞—Å—Ç—ã—Ö –æ–ø–µ—á–∞—Ç–æ–∫
        fallback_corrections = {
            "–∫–∞–ª–ª": "–∫–∞–ª",
            "—Ñ—Å–ø–µ—Ä–º–∞": "—Å–ø–µ—Ä–º–∞",
            "–∏–∫–∞–ª": "–∫–∞–ª",
            "—Ñ–µ–∫–∞–ª–∏–∏–∏": "—Ñ–µ–∫–∞–ª–∏–∏",
        }

        def process_token(token: str) -> str:
            if token.isdigit():
                return token
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–±–ª–∏—Ü—É –æ–ø–µ—á–∞—Ç–æ–∫
            lower_token = token.lower()
            if lower_token in fallback_corrections:
                return fallback_corrections[lower_token]
            
            # –ó–∞—Ç–µ–º –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å pymorphy
            try:
                parsed = self.morph.parse(token)
                if parsed and parsed[0].tag.POS != 'UNKN':
                    return parsed[0].normal_form
            except Exception:
                pass
            
            return token

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω
        tokens = re.findall(r'\b[\w\-]+\b', query, flags=re.UNICODE)
        processed_tokens = [process_token(token) for token in tokens]
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        pattern = r'\b[\w\-]+\b'
        result = re.sub(pattern, lambda m: processed_tokens.pop(0), query)
        
        return self._normalize_text(result)

    # ==================== –¢–†–ê–ù–°–õ–ò–¢–ï–†–ê–¶–ò–Ø ====================

    def _generate_transliteration_variants(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤."""
        if not text:
            return []

        # –¢–∞–±–ª–∏—Ü—ã —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
        eng_to_rus = {
            'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì', 
            'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 
            'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£', 
            'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
        }

        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E', 
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M', 
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U', 
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–´': 'Y', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
        }

        variants = set()
        text_upper = text.upper()

        def is_valid_variant(variant: str) -> bool:
            """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã–º."""
            if not variant or variant == text:
                return False
            
            len_diff = abs(len(variant) - len(text))
            if len_diff > 2:
                return False
            
            # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            if len(text) <= 4:
                score = fuzz.ratio(text_upper, variant.upper())
                return score >= 90 and len_diff <= 1
            else:
                score = fuzz.ratio(text_upper, variant.upper())
                return score >= 65

        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', text):
            eng = ''.join(rus_to_eng.get(ch, ch) for ch in text)
            if is_valid_variant(eng):
                variants.update([eng, eng.lower(), eng.upper()])

        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        elif re.search(r'[A-Za-z]', text):
            rus = ''.join(eng_to_rus.get(ch, ch) for ch in text)
            if is_valid_variant(rus):
                variants.update([rus, rus.lower(), rus.upper()])

        return [v for v in variants if v and len(v) >= 2]

    # ==================== –ì–ï–ù–ï–†–ê–¶–ò–Ø –§–û–†–ú ====================

    def _generate_all_abbreviation_forms(self, abbr: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã."""
        if not abbr:
            return []
        
        forms = set()
        forms.update([abbr, abbr.upper(), abbr.lower()])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–µ–∫ –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º
        if '.' in abbr:
            no_dots = abbr.replace('.', '')
            forms.update([no_dots, no_dots.upper()])
        elif abbr.isupper() and 2 <= len(abbr) <= 6:
            with_dots = '.'.join(list(abbr)) + '.'
            forms.update([with_dots, with_dots.lower()])

        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã
        try:
            translits = self._generate_transliteration_variants(abbr)
            forms.update(translits)
        except Exception:
            pass

        return [f for f in forms if 1 < len(f) <= 30]

    def _generate_medical_term_forms(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤."""
        if not text:
            return []
        
        forms = set()
        text_stripped = text.strip()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
        forms.update([
            text_stripped,
            text_stripped.lower(),
            text_stripped.upper(),
            text_stripped.capitalize()
        ])

        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        try:
            forms.update(self._generate_transliteration_variants(text_stripped))
        except Exception:
            pass

        words = re.findall(r'\b[\w\-]+\b', text_stripped, flags=re.UNICODE)
        
        if len(words) > 1:
            # –ú–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã–π —Ç–µ—Ä–º–∏–Ω - –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã
            for w in words:
                try:
                    parsed = self.morph.parse(w)
                    if parsed and parsed[0].tag.POS != 'UNKN':
                        forms.add(parsed[0].normal_form)
                except Exception:
                    forms.add(w)
            
            # –§—Ä–∞–∑—ã –∏–∑ 2-3 —Å–ª–æ–≤
            for i in range(len(words)):
                for j in range(i + 1, min(len(words), i + 3)):
                    phrase = ' '.join(words[i:j])
                    if len(phrase) >= 2:
                        forms.update([phrase, phrase.lower(), phrase.capitalize()])
        else:
            # –û–¥–Ω–æ—Å–ª–æ–≤–Ω—ã–π —Ç–µ—Ä–º–∏–Ω - –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã
            w = words[0] if words else text_stripped
            try:
                parsed = self.morph.parse(w)
                if parsed:
                    p = parsed[0]
                    norm = p.normal_form
                    forms.add(norm)
                    
                    # –ü–∞–¥–µ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã
                    for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                        try:
                            inf = p.inflect({case})
                            if inf:
                                forms.add(inf.word)
                        except Exception:
                            continue
            except Exception:
                forms.add(w)

        return [f for f in forms if f and len(f) >= 2]

    # ==================== –ó–ê–ì–†–£–ó–ö–ê –°–õ–û–í–ê–†–ï–ô ====================

    def _load_all_dictionaries(self) -> Dict[str, Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ Excel —Ñ–∞–π–ª–∞."""
        all_dicts = {
            'vet_abbr': {}, 'vet_full': {},
            'disease_abbr': {}, 'disease_full': {},
            'pcr_abbr': {}, 'pcr_full': {}
        }
        
        try:
            file_path = Path(self.excel_file)
            if not file_path.exists():
                logger.warning(f"Excel file not found: {self.excel_file}")
                return all_dicts

            with pd.ExcelFile(self.excel_file, engine='openpyxl') as xls:
                # –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                if '–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∏–∏' in xls.sheet_names:
                    df_vet = pd.read_excel(xls, sheet_name='–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–∏–∏')
                    self._process_vet_abbreviations(df_vet, all_dicts)
                
                # –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π
                if '–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π' in xls.sheet_names:
                    df_dis = pd.read_excel(xls, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π')
                    self._process_diseases(df_dis, all_dicts)
                
                # –ü–¶–† —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
                if '–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†' in xls.sheet_names:
                    df_pcr = pd.read_excel(xls, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†')
                    self._process_pcr_abbreviations(df_pcr, all_dicts)

            self._check_dictionary_conflicts(all_dicts)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä–µ–π: {e}")

        return all_dicts

    def _check_dictionary_conflicts(self, all_dicts: Dict[str, Dict]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –º–µ–∂–¥—É —Å–ª–æ–≤–∞—Ä—è–º–∏."""
        conflicts = []
        all_abbr_keys = {}
        
        for dict_name in ['vet_abbr', 'disease_abbr', 'pcr_abbr']:
            for key in all_dicts[dict_name].keys():
                if key in all_abbr_keys:
                    conflicts.append(f"–ö–æ–Ω—Ñ–ª–∏–∫—Ç: '{key}' –≤ {all_abbr_keys[key]} –∏ {dict_name}")
                else:
                    all_abbr_keys[key] = dict_name
        
        if conflicts:
            logger.warning(f"–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤: {len(conflicts)}")
            for c in conflicts[:10]:
                logger.warning(c)

    def _process_vet_abbreviations(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã."""
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row.get('–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                rus_name = self._safe_string_conversion(row.get('–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞', ''))
                eng_name = self._safe_string_conversion(row.get('–ê–Ω–≥–ª–∏–π—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ', ''))

                if not abbr or not rus_name:
                    continue

                abbr_variants = self._split_variants(abbr, '/')
                rus_variants = self._split_variants(rus_name, '/')

                for a in abbr_variants:
                    abbr_forms = self._generate_all_abbreviation_forms(a)
                    for af in abbr_forms:
                        if af in all_dicts['vet_abbr']:
                            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
                            existing = all_dicts['vet_abbr'][af]
                            existing['russian_names'] = list(set(existing.get('russian_names', []) + rus_variants))
                        else:
                            all_dicts['vet_abbr'][af] = {
                                'type': 'vet_abbr',
                                'russian_names': rus_variants,
                                'original_abbr': a,
                                'all_abbr_forms': abbr_forms
                            }

                # –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                for r in rus_variants:
                    forms = self._generate_medical_term_forms(r)
                    for f in forms:
                        if f in all_dicts['vet_full']:
                            existing = all_dicts['vet_full'][f]
                            existing['abbreviations'] = list(set(existing.get('abbreviations', []) + abbr_variants))
                        else:
                            all_dicts['vet_full'][f] = {
                                'type': 'vet_full',
                                'abbreviations': abbr_variants,
                                'original_name': r,
                                'all_abbr_forms': abbr_forms
                            }

                # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                if eng_name:
                    eng_forms = self._generate_medical_term_forms(eng_name)
                    for f in eng_forms:
                        all_dicts['vet_full'][f] = {
                            'type': 'vet_english',
                            'abbreviations': abbr_variants,
                            'original_name': eng_name,
                            'all_abbr_forms': abbr_forms
                        }
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–µ—Ç. –∞–±–±—Ä.: {e}")

    def _process_pcr_abbreviations(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ü–¶–† —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è."""
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row.get('–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                full = self._safe_string_conversion(row.get('–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞', ''))
                
                if not abbr or not full:
                    continue
                    
                abbr_variants = self._split_variants(abbr, '/')
                full_vars = self._split_variants(full, '/')
                
                for a in abbr_variants:
                    abbr_forms = self._generate_all_abbreviation_forms(a)
                    for af in abbr_forms:
                        all_dicts['pcr_abbr'][af] = {
                            'type': 'pcr_abbr',
                            'full_names': full_vars,
                            'original_abbr': a,
                            'all_abbr_forms': abbr_forms
                        }
                
                for fv in full_vars:
                    full_forms = self._generate_medical_term_forms(fv)
                    for ff in full_forms:
                        all_dicts['pcr_full'][ff] = {
                            'type': 'pcr_full',
                            'abbreviations': abbr_variants,
                            'original_name': fv,
                            'all_abbr_forms': [x for a in abbr_variants for x in self._generate_all_abbreviation_forms(a)]
                        }
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ü–¶–†: {e}")

    def _process_diseases(self, df: pd.DataFrame, all_dicts: Dict):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –±–æ–ª–µ–∑–Ω–µ–π."""
        for _, row in df.iterrows():
            try:
                official_name = self._safe_string_conversion(row.get('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º', ''))
                colloquial = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ', ''))
                abbr = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))

                if not official_name:
                    continue

                official_forms = self._generate_medical_term_forms(official_name)
                abbr_variants = []
                all_abbr_forms_for_disease = []

                # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –±–æ–ª–µ–∑–Ω–µ–π
                if abbr:
                    abbr_variants = self._split_variants(abbr, ',')
                    for a in abbr_variants:
                        abbr_forms = self._generate_all_abbreviation_forms(a)
                        all_abbr_forms_for_disease.extend(abbr_forms)
                        for af in abbr_forms:
                            all_dicts['disease_abbr'][af] = {
                                'type': 'disease_abbr',
                                'official_name': official_name,
                                'original_abbr': a,
                                'all_abbr_forms': abbr_forms
                            }

                # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                for f in official_forms:
                    all_dicts['disease_full'][f] = {
                        'type': 'disease_full',
                        'official_name': official_name,
                        'abbreviations': abbr_variants,
                        'original_name': official_name,
                        'all_abbr_forms': all_abbr_forms_for_disease
                    }

                # –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
                if colloquial:
                    colloquial_variants = self._split_variants(colloquial, ',')
                    for c in colloquial_variants:
                        c_forms = self._generate_medical_term_forms(c)
                        for cf in c_forms:
                            all_dicts['disease_full'][cf] = {
                                'type': 'disease_colloquial',
                                'official_name': official_name,
                                'abbreviations': abbr_variants,
                                'original_name': c,
                                'all_abbr_forms': all_abbr_forms_for_disease
                            }
                            
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª–µ–∑–Ω–µ–π: {e}")

    # ==================== –ü–û–ò–°–ö –°–û–í–ü–ê–î–ï–ù–ò–ô ====================

    def _build_fuzzy_search_indexes(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –Ω–µ—á–µ—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
        self.fuzzy_full_names = []
        self.fuzzy_abbreviations = []

        # –ü–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        for dict_name in ['vet_full', 'disease_full', 'pcr_full']:
            dictionary = self.all_dictionaries.get(dict_name, {})
            for key, data in dictionary.items():
                if key and len(key) >= 2:
                    priority = 5 if data.get('type') == 'disease_colloquial' else min(len(key.split()), 3)
                    self.fuzzy_full_names.append((key, dict_name, priority))

        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        for dict_name in ['vet_abbr', 'disease_abbr', 'pcr_abbr']:
            dictionary = self.all_dictionaries.get(dict_name, {})
            for key, data in dictionary.items():
                if key and len(key) >= 2:
                    self.fuzzy_abbreviations.append((key, dict_name, 1))
                    for form in data.get('all_abbr_forms', []):
                        if form and len(form) >= 2:
                            self.fuzzy_abbreviations.append((form, dict_name, 1))

        self.fuzzy_full_names.sort(key=lambda x: x[2], reverse=True)
        logger.info(f"–ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã: full_names={len(self.fuzzy_full_names)}, abbrs={len(self.fuzzy_abbreviations)}")

    def _is_position_used(self, start: int, end: int, used_positions: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ —É–∂–µ –ø–æ–∑–∏—Ü–∏—è."""
        for used_start, used_end in used_positions:
            if not (end <= used_start or start >= used_end):
                return True
        return False

    def _find_existing_expansions(self, query: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ."""
        expansions = []
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "—Ç–µ–∫—Å—Ç (—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞)"
        pattern = r'(\b[\w\-]+\b)\s*\(([^)]+)\)'
        matches = re.finditer(pattern, query)
        
        for match in matches:
            expansions.append({
                'text': match.group(1).strip(),
                'expansion': match.group(2).strip(),
                'start': match.start(),
                'end': match.end()
            })
        
        return expansions

    def _is_part_of_existing_expansion(self, start: int, end: int, existing_expansions: List[Dict]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
        for expansion in existing_expansions:
            if start >= expansion['start'] and end <= expansion['end']:
                return True
        return False

    def _is_part_of_expansion(self, text: str, start: int, end: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è."""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∏—â–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É –ø–µ—Ä–µ–¥ –ø–æ–∑–∏—Ü–∏–µ–π –∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é –ø–æ—Å–ª–µ
        text_before = text[:start]
        text_after = text[end:]
        
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥ –ø–æ–∑–∏—Ü–∏–µ–π –µ—Å—Ç—å –Ω–µ–∑–∞–∫—Ä—ã—Ç–∞—è —Å–∫–æ–±–∫–∞
        open_brackets_before = text_before.count('(')
        close_brackets_before = text_before.count(')')
        
        if open_brackets_before > close_brackets_before:
            return True
        
        # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø–∞—Ä—ã —Å–∫–æ–±–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ö–≤–∞—Ç—ã–≤–∞—é—Ç —ç—Ç—É –ø–æ–∑–∏—Ü–∏—é
        bracket_pairs = []
        stack = []
        
        for i, char in enumerate(text):
            if char == '(':
                stack.append(i)
            elif char == ')' and stack:
                start_bracket = stack.pop()
                bracket_pairs.append((start_bracket, i))
    
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–∞—è –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ –∫–∞–∫–∏—Ö-–ª–∏–±–æ —Å–∫–æ–±–æ–∫
        for bracket_start, bracket_end in bracket_pairs:
            if bracket_start <= start <= bracket_end:
                return True
        
        return False

    def _would_cause_duplication(self, text: str, start: int, end: int, replacement: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ –≤—ã–∑–æ–≤–µ—Ç –ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è."""
        original = text[start:end]
        
        context_start = max(0, start - len(replacement) - 10)
        context_end = min(len(text), end + len(replacement) + 10)
        context = text[context_start:context_end]
        
        duplication_patterns = [
            f"{replacement} {replacement}",
            f"{replacement} {original}",
            f"{original} {replacement}",
            f"{replacement}({original})",
            f"{original}({replacement})",
            f"({replacement}) {original}",
            f"({original}) {replacement}",
        ]
        
        return any(pattern in context for pattern in duplication_patterns)

    def _generate_search_terms(self, word: str) -> Set[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å–ª–æ–≤–∞."""
        terms = {word, word.lower(), word.upper(), word.capitalize()}
        
        try:
            parsed = self.morph.parse(word)
            if parsed:
                p = parsed[0]
                norm = p.normal_form
                terms.add(norm)
                terms.add(norm.lower())
                
                # –ü–∞–¥–µ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã
                if p.tag.POS in ['NOUN', 'ADJF', 'ADJS', 'ADJ']:
                    for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                        try:
                            inf = p.inflect({case})
                            if inf:
                                terms.add(inf.word)
                        except Exception:
                            continue
        except Exception:
            pass
            
        return terms

    def _find_exact_matches(self, query: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è n-gram."""
        matches = []
        words = query.split()
        
        for n in [3, 2, 1]:  # –°–Ω–∞—á–∞–ª–∞ 3-gram, –∑–∞—Ç–µ–º 2-gram, –∑–∞—Ç–µ–º 1-gram
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i + n])
                start = query.find(ngram)
                end = start + len(ngram)
                
                if (start < 0 or 
                    self._is_position_used(start, end, used_positions) or
                    self._is_part_of_existing_expansion(start, end, existing_expansions)):
                    continue
                
                # –ü–æ–∏—Å–∫ –≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤–∞—Ä—è—Ö –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
                for dict_name in ['vet_full', 'disease_full', 'pcr_full']:
                    dictionary = self.all_dictionaries.get(dict_name, {})
                    if ngram in dictionary:
                        matches.append({
                            'start': start, 'end': end, 'found_text': ngram,
                            'dict_name': dict_name, 'data': dictionary[ngram],
                            'match_type': 'exact_ngram'
                        })
                        used_positions.add((start, end))
                        self.match_stats['exact_ngram'] += 1
                        break
        
        return matches

    def _find_single_word_matches(self, query: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–ª–æ–≤."""
        matches = []
        words = query.split()
        
        for w in words:
            start = query.find(w)
            end = start + len(w)
            
            if (start < 0 or len(w) < self.min_word_length or 
                w.lower() in self.ignore_words or 
                self._is_position_used(start, end, used_positions) or
                self._is_part_of_existing_expansion(start, end, existing_expansions)):
                continue
            
            search_terms = self._generate_search_terms(w)
            
            # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä—è—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
            abbr_found = False
            for dict_name in ['vet_abbr', 'disease_abbr', 'pcr_abbr']:
                dictionary = self.all_dictionaries.get(dict_name, {})
                for st in search_terms:
                    if st in dictionary:
                        matches.append({
                            'start': start, 'end': end, 'found_text': w,
                            'dict_name': dict_name, 'data': dictionary[st],
                            'match_type': 'exact_single'
                        })
                        used_positions.add((start, end))
                        self.match_stats['exact_single'] += 1
                        abbr_found = True
                        break
                if abbr_found:
                    break
            
            if abbr_found:
                continue
            
            # –ó–∞—Ç–µ–º –≤ —Å–ª–æ–≤–∞—Ä—è—Ö –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
            for dict_name in ['vet_full', 'disease_full', 'pcr_full']:
                dictionary = self.all_dictionaries.get(dict_name, {})
                for st in search_terms:
                    if st in dictionary:
                        matches.append({
                            'start': start, 'end': end, 'found_text': w,
                            'dict_name': dict_name, 'data': dictionary[st],
                            'match_type': 'exact_single'
                        })
                        used_positions.add((start, end))
                        self.match_stats['exact_single'] += 1
                        break
                if any(m['found_text'] == w for m in matches):
                    break
        
        return matches

    def _find_fuzzy_matches(self, query: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–µ—á–µ—Ç–∫–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è."""
        matches = []
        words = query.split()

        full_candidates = [item[0] for item in self.fuzzy_full_names]
        abbr_candidates = [item[0] for item in self.fuzzy_abbreviations]

        for w in words:
            start = query.find(w)
            end = start + len(w)
            
            if (start < 0 or len(w) < self.min_word_length or 
                w.lower() in self.ignore_words or 
                self._is_position_used(start, end, used_positions) or
                self._is_part_of_existing_expansion(start, end, existing_expansions)):
                continue

            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥
            if len(w) <= 4:
                local_threshold = 90
            elif len(w) <= 6:
                local_threshold = 80
            else:
                local_threshold = self.fuzzy_threshold

            # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –ø–æ–ª–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
            try:
                if full_candidates:
                    top = process.extractBests(w, full_candidates, scorer=fuzz.ratio, 
                                             score_cutoff=local_threshold, limit=5)
                    for match_text, score in top:
                        for original_key, dict_name, _ in self.fuzzy_full_names:
                            if original_key == match_text:
                                data = self.all_dictionaries.get(dict_name, {}).get(original_key)
                                if data:
                                    matches.append({
                                        'start': start, 'end': end, 'found_text': w,
                                        'dict_name': dict_name, 'data': data,
                                        'match_type': 'fuzzy_full', 'score': score
                                    })
                                    used_positions.add((start, end))
                                break
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ fuzzy full: {e}")

            # –ù–µ—á–µ—Ç–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º
            if not self._is_position_used(start, end, used_positions):
                try:
                    if abbr_candidates:
                        top_ab = process.extractBests(w.upper(), abbr_candidates, scorer=fuzz.ratio,
                                                    score_cutoff=local_threshold, limit=3)
                        for match_text, score in top_ab:
                            for original_key, dict_name, _ in self.fuzzy_abbreviations:
                                if original_key == match_text:
                                    data = self.all_dictionaries.get(dict_name, {}).get(original_key)
                                    if data:
                                        matches.append({
                                            'start': start, 'end': end, 'found_text': w,
                                            'dict_name': dict_name, 'data': data,
                                            'match_type': 'fuzzy_abbr', 'score': score
                                        })
                                        used_positions.add((start, end))
                                    break
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ fuzzy abbr: {e}")

        return matches

    def _find_all_matches(self, query: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏."""
        used_positions = set()
        matches = []
        
        # –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ –∏—Å–∫–ª—é—á–∞–µ–º –∏—Ö –∏–∑ –ø–æ–∏—Å–∫–∞
        existing_expansions = self._find_existing_expansions(query)
        for expansion in existing_expansions:
            used_positions.add((expansion['start'], expansion['end']))
    
        # –ü–æ–∏—Å–∫ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        exact_matches = self._find_exact_matches(query, used_positions, existing_expansions)
        single_matches = self._find_single_word_matches(query, used_positions, existing_expansions)
        fuzzy_matches = self._find_fuzzy_matches(query, used_positions, existing_expansions)
        
        matches.extend(exact_matches)
        matches.extend(single_matches)
        matches.extend(fuzzy_matches)

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –¥–ª–∏–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏, —Ç–æ—á–Ω—ã–µ –≤—ã—à–µ –Ω–µ—á–µ—Ç–∫–∏—Ö
        matches.sort(key=lambda x: (
            -(x['end'] - x['start']),  # –î–ª–∏–Ω–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é
            x['start'],  # –ü–æ–∑–∏—Ü–∏—è –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é  
            0 if x.get('match_type', '').startswith('exact') else 1  # –¢–æ—á–Ω—ã–µ –≤—ã—à–µ
        ))
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        return matches

    # ==================== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ò–ô ====================

    def _create_expanded_text(self, found_text: str, dict_name: str, data: Dict, original_query: str = "") -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–ø—Ä–æ—Å–µ
        if original_query:
            found_pos = original_query.find(found_text)
            if found_pos != -1:
                # –°–º–æ—Ç—Ä–∏–º, —á—Ç–æ –∏–¥–µ—Ç –ø–æ—Å–ª–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
                after_text = original_query[found_pos + len(found_text):].strip()
                if after_text.startswith('(') and ')' in after_text:
                    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —É–∂–µ –µ—Å—Ç—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    return found_text
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
        if dict_name == 'vet_abbr':
            russian_names = data.get('russian_names', [])
            if russian_names and russian_names[0] != found_text:
                return f"{found_text} ({russian_names[0]})"
                
        elif dict_name == 'vet_full':
            abbrs = data.get('abbreviations', [])
            if abbrs and abbrs[0] != found_text:
                return f"{found_text} ({abbrs[0]})"
                
        elif dict_name == 'disease_abbr':
            official = data.get('official_name', '')
            if official and official != found_text:
                return f"{found_text} ({official})"
                
        elif dict_name == 'disease_full':
            abbrs = data.get('abbreviations', [])
            if abbrs and abbrs[0] != found_text:
                return f"{found_text} ({abbrs[0]})"
            else:
                official = data.get('official_name', '')
                if official and official != found_text:
                    return f"{found_text} ({official})"
                    
        elif dict_name == 'pcr_abbr':
            fulls = data.get('full_names', [])
            if fulls and fulls[0] != found_text:
                return f"{found_text} ({fulls[0]})"
                
        elif dict_name == 'pcr_full':
            abbrs = data.get('abbreviations', [])
            if abbrs and abbrs[0] != found_text:
                return f"{found_text} ({abbrs[0]})"
        
        return found_text

    def _apply_expansions(self, query: str, matches: List[Dict]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫ –∑–∞–ø—Ä–æ—Å—É, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è."""
        if not matches:
            return query

        result = query
        offset = 0
        used_positions = set()
        
        for match in matches:
            start = match['start'] + offset
            end = match['end'] + offset
            found_text = match['found_text']
            dict_name = match['dict_name']
            data = match['data']

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞
            if self._is_position_used(start, end, used_positions):
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            if self._is_part_of_expansion(result, start, end):
                continue

            expanded = self._create_expanded_text(found_text, dict_name, data, query)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–æ —Ç–µ–∫—Å—Ç
            if expanded == found_text:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
            if self._would_cause_duplication(result, start, end, expanded):
                continue

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            result = result[:start] + expanded + result[end:]
            offset += len(expanded) - len(found_text)
            
            # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
            used_positions.add((start, start + len(expanded)))
            
            logger.debug(f"–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{found_text}' -> '{expanded}'")

        return result

    # ==================== –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–° ====================

    def expand_query(self, query: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞."""
        logger.info(f"üì• –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{query}'")
        
        if not query:
            return query

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if query in self.processed_queries:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            return self.processed_queries[query]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        query = self._normalize_text(query)

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫
        corrected_query = self._fix_typos(query)
        if corrected_query != query:
            logger.info(f"üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω—ã –æ–ø–µ—á–∞—Ç–∫–∏: '{query}' -> '{corrected_query}'")
            query = corrected_query

        # –ü–æ–∏—Å–∫ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
        matches = self._find_all_matches(query)
        result = self._apply_expansions(query, matches)

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        if len(self.processed_queries) > self.MAX_CACHE_SIZE:
            self.processed_queries.clear()
        self.processed_queries[query] = result

        if result != query:
            logger.info(f"üì§ –ü–æ—Å–ª–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: '{result}'")
        else:
            logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{result}'")
            
        return result


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
abbreviation_expander = UnifiedAbbreviationExpander()

def expand_query_with_abbreviations(query: str) -> str:
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–Ω–µ—à–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è."""
    return abbreviation_expander.expand_query(query)