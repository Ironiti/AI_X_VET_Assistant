import re
import pandas as pd
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class DiseasesProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–∞–±–ª–∏—Ü—ã –±–æ–ª–µ–∑–Ω–µ–π - –°–ê–ú –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
    
    def __init__(self, morph_analyzer):
        self.morph = morph_analyzer
    
    def _safe_string_conversion(self, value) -> str:
        if pd.isna(value) or value is None:
            return ""
        return str(value).strip()
    
    def _split_variants(self, text: str, delimiter: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é (–∑–∞–ø—è—Ç–∞—è –¥–ª—è –±–æ–ª–µ–∑–Ω–µ–π)"""
        if not text:
            return []
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        variants = [v.strip() for v in re.split(r'\s*' + re.escape(delimiter) + r'\s*', text) if v.strip()]
        return variants if variants else [text]
    
    def _generate_all_abbreviation_forms(self, abbr: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –±–æ–ª–µ–∑–Ω–µ–π (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è + —Ä–µ–≥–∏—Å—Ç—Ä—ã)"""
        if not abbr:
            return []
        
        forms = set()
        forms.update([abbr, abbr.upper(), abbr.lower(), abbr.capitalize()])
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–´': 'Y', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
        }
        
        eng_to_rus = {
            'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì', 
            'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 
            'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£', 
            'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
        }
        
        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', abbr):
            eng = ''.join(rus_to_eng.get(ch.upper(), ch) for ch in abbr).upper()
            if eng != abbr.upper():
                forms.update([eng, eng.lower()])
        
        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        elif re.search(r'[A-Za-z]', abbr):
            rus = ''.join(eng_to_rus.get(ch.upper(), ch) for ch in abbr).upper()
            if rus != abbr.upper():
                forms.update([rus, rus.lower()])
        
        return [f for f in forms if f and 1 < len(f) <= 30]
    
    def _generate_disease_forms(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –±–æ–ª–µ–∑–Ω–µ–π (—Å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–µ–π)"""
        if not text:
            return []
        
        forms = set()
        text_stripped = text.strip()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã
        forms.update([
            text_stripped,
            text_stripped.lower(),
            text_stripped.upper(),
        ])
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–´': 'Y', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA',
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'e',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'kh', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
            '—ã': 'y', '—ç': 'e', '—é': 'yu', '—è': 'ya'
        }
        
        # –†—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è -> –∞–Ω–≥–ª–∏–π—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', text_stripped):
            # –ü—Ä–æ—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
            eng_translit = ''.join(rus_to_eng.get(ch, ch) for ch in text_stripped)
            if eng_translit != text_stripped:
                forms.update([eng_translit, eng_translit.lower(), eng_translit.upper()])
            
            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
            words = text_stripped.split()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –æ—Ç–¥–µ–ª—å–Ω–æ
            processed_words = []
            for word in words:
                word_forms = {word.lower()}
                try:
                    parsed = self.morph.parse(word)[0]
                    if parsed.tag.POS in ['NOUN', 'ADJF', 'ADJS']:
                        word_forms.add(parsed.normal_form)
                        # –ü–∞–¥–µ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã
                        for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                            try:
                                inflected = parsed.inflect({case})
                                if inflected:
                                    word_forms.add(inflected.word)
                            except:
                                pass
                except:
                    word_forms.add(word.lower())
                processed_words.append(word_forms)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã —Å–ª–æ–≤
            if len(processed_words) == 1:
                forms.update(processed_words[0])
            else:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º
                from itertools import product
                for combination in product(*processed_words):
                    phrase = ' '.join(combination)
                    forms.add(phrase)
        
        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è -> —Ä—É—Å—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        elif re.search(r'[A-Za-z]', text_stripped):
            eng_to_rus = {
                'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì', 
                'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 
                'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£', 
                'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó',
                'a': '–∞', 'b': '–≤', 'c': '—Å', 'd': '–¥', 'e': '–µ', 'f': '—Ñ', 'g': '–≥', 
                'h': '—Ö', 'i': '–∏', 'j': '–¥–∂', 'k': '–∫', 'l': '–ª', 'm': '–º', 'n': '–Ω', 
                'o': '–æ', 'p': '–ø', 'q': '–∫', 'r': '—Ä', 's': '—Å', 't': '—Ç', 'u': '—É', 
                'v': '–≤', 'w': '–≤', 'x': '–∫—Å', 'y': '–∏', 'z': '–∑'
            }
            
            rus_translit = ''.join(eng_to_rus.get(ch, ch) for ch in text_stripped)
            if rus_translit != text_stripped:
                forms.update([rus_translit, rus_translit.lower(), rus_translit.upper()])

        return [f for f in forms if f and len(f) >= 2]
    
    def _find_existing_expansions(self, query: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ (—á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ)"""
        expansions = []
        pattern = r'(\b[\w\-]+\b)\s*\(([^)]+)\)'
        matches = re.finditer(pattern, query)
        
        for match in matches:
            expansions.append({
                'text': match.group(1).strip(),
                'expansion': match.group(2).strip(),
                'start': match.start(),
                'end': match.end()
            })
        
        return expansions
    
    def _is_position_used(self, start: int, end: int, used_positions: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ —É–∂–µ –ø–æ–∑–∏—Ü–∏—è"""
        for used_start, used_end in used_positions:
            if not (end <= used_start or start >= used_end):
                return True
        return False
    
    def _is_part_of_existing_expansion(self, start: int, end: int, existing_expansions: List[Dict]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        for expansion in existing_expansions:
            if start >= expansion['start'] and end <= expansion['end']:
                return True
        return False
    
    def _find_matches_in_query(self, query: str, search_dict: Dict, dict_type: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏"""
        matches = []
        words = query.split()
        
        for n in range(min(4, len(words)), 0, -1):
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i + n])
                start = query.find(ngram)
                end = start + len(ngram)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                if (start < 0 or 
                    self._is_position_used(start, end, used_positions) or
                    self._is_part_of_existing_expansion(start, end, existing_expansions)):
                    continue
                
                # üîÑ –ò–ì–ù–û–†–ò–†–£–ï–ú –†–ï–ì–ò–°–¢–† –ü–†–ò –ü–û–ò–°–ö–ï
                ngram_lower = ngram.lower()
                found_in_dict = False
                dict_data = None
                
                # –ò—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
                for key, value in search_dict.items():
                    if key.lower() == ngram_lower:
                        found_in_dict = True
                        dict_data = value
                        break
                
                if found_in_dict and dict_data:
                    matches.append({
                        'start': start, 
                        'end': end, 
                        'found_text': ngram,
                        'data': dict_data,
                        'dict_type': dict_type,
                        'word_count': n
                    })
                    used_positions.add((start, end))
        
        return matches
    
    def _apply_disease_expansions(self, query: str, matches: List[Dict]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –±–æ–ª–µ–∑–Ω–µ–π"""
        if not matches:
            return query
        
        result = query
        offset = 0
        
        for match in matches:
            start = match['start'] + offset
            end = match['end'] + offset
            found_text = match['found_text']
            data = match['data']
            dict_type = match['dict_type']
            
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º
            expanded = found_text
            
            if dict_type == 'disease_abbr':
                original_name = data.get('original_name', '')
                if original_name and original_name != found_text:
                    expanded = f"{found_text} ({original_name})"
                    
            elif dict_type == 'disease_full':
                original_name = data.get('original_name', '')
                if original_name and original_name != found_text:
                    expanded = f"{found_text} ({original_name})"
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
            if expanded != found_text:
                result = result[:start] + expanded + result[end:]
                offset += len(expanded) - len(found_text)
                logger.debug(f"üîç –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–æ–ª–µ–∑–Ω–∏: '{found_text}' -> '{expanded}'")
        
        return result
    
    def process_table(self, df: pd.DataFrame) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        logger.info("üîß –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É –±–æ–ª–µ–∑–Ω–µ–π")
        
        disease_abbr = {}
        disease_full = {}
        
        for _, row in df.iterrows():
            try:
                official = self._safe_string_conversion(row.get('–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º', ''))
                colloquial = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ', ''))
                abbr = self._safe_string_conversion(row.get('–†–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                
                if not official:
                    continue
                
                # üîÑ –†–ê–ó–î–ï–õ–Ø–ï–ú –í–ê–†–ò–ê–ù–¢–´ –ü–û –ó–ê–ü–Ø–¢–û–ô - –í–ê–ñ–ù–û –¥–ª—è —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
                abbr_variants = self._split_variants(abbr, ',') if abbr else []
                colloquial_variants = self._split_variants(colloquial, ',') if colloquial else []
                
                # –í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π –±–æ–ª–µ–∑–Ω–∏ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ + —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ)
                all_names = [official] + colloquial_variants
                
                # 1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –Ω–∞–∑–≤–∞–Ω–∏—è (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ + —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ)
                for name in all_names:
                    name_forms = self._generate_disease_forms(name)
                    for nf in name_forms:
                        if nf not in disease_full:
                            disease_full[nf] = {
                                'original_name': official,  # –í—Å–µ–≥–¥–∞ —Å—Å—ã–ª–∞–µ–º—Å—è –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                                'type': 'disease_official' if name == official else 'disease_colloquial',
                                'found_variant': name
                            }
                
                # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –±–æ–ª–µ–∑–Ω–µ–π
                for a in abbr_variants:
                    abbr_forms = self._generate_all_abbreviation_forms(a)
                    for af in abbr_forms:
                        if af not in disease_abbr:
                            disease_abbr[af] = {
                                'original_name': official,
                                'original_abbr': a,
                                'type': 'disease_abbr'
                            }
                            
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –≤ –±–æ–ª–µ–∑–Ω—è—Ö: {e}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ç–ª–∞–¥–∫–∞ - –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –¥–æ–±–∞–≤–∏–ª–æ—Å—å –¥–ª—è –í–ò–ö
        logger.debug("üîç –ü–æ–∏—Å–∫ –í–ò–ö –≤ disease_full:")
        for key in disease_full:
            if '–≤–∏–∫' in key.lower():
                logger.debug(f"   –ù–∞–π–¥–µ–Ω–æ: '{key}' -> {disease_full[key]}")
        
        logger.info(f"‚úÖ –ë–æ–ª–µ–∑–Ω–∏: {len(disease_abbr)} –∞–±–±—Ä, {len(disease_full)} –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        return {
            'disease_abbr': disease_abbr,
            'disease_full': disease_full
        }
    

    def expand_query(self, query: str, disease_dicts: Dict) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –±–æ–ª–µ–∑–Ω–µ–π –∫ –∑–∞–ø—Ä–æ—Å—É"""
        if not query:
            return query
        
        # –ù–∞—Ö–æ–¥–∏–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ
        existing_expansions = self._find_existing_expansions(query)
        used_positions = set((exp['start'], exp['end']) for exp in existing_expansions)
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –æ–±–æ–∏—Ö —Å–ª–æ–≤–∞—Ä—è—Ö, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        all_matches = []
        all_matches.extend(self._find_matches_in_query(query, disease_dicts['disease_abbr'], 'disease_abbr', used_positions, existing_expansions))
        all_matches.extend(self._find_matches_in_query(query, disease_dicts['disease_full'], 'disease_full', used_positions, existing_expansions))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏) –∏ –ø–æ–∑–∏—Ü–∏–∏
        all_matches.sort(key=lambda x: (-x['word_count'], x['start']))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        return self._apply_disease_expansions(query, all_matches)