import re
import pandas as pd
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class PCRProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–∞–±–ª–∏—Ü—ã –ü–¶–†-—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π - –°–ê–ú –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
    
    def __init__(self, morph_analyzer):
        self.morph = morph_analyzer
    
    def _safe_string_conversion(self, value) -> str:
        if pd.isna(value) or value is None:
            return ""
        return str(value).strip()
    
    def _split_variants(self, text: str, delimiter: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é"""
        if not text:
            return []
        variants = [v.strip() for v in re.split(re.escape(delimiter), text) if v.strip()]
        return variants if variants else [text]
    
    def _generate_all_abbreviation_forms(self, abbr: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –ü–¶–†-–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è + —Ä–µ–≥–∏—Å—Ç—Ä—ã)"""
        if not abbr:
            return []
        
        forms = set()
        forms.update([abbr, abbr.upper(), abbr.lower(), abbr.capitalize()])
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–´': 'Y', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
        }
        
        eng_to_rus = {
            'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì', 
            'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 
            'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£', 
            'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
        }
        
        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', abbr):
            eng = ''.join(rus_to_eng.get(ch.upper(), ch) for ch in abbr).upper()
            if eng != abbr.upper():
                forms.update([eng, eng.lower()])
        
        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        elif re.search(r'[A-Za-z]', abbr):
            rus = ''.join(eng_to_rus.get(ch.upper(), ch) for ch in abbr).upper()
            if rus != abbr.upper():
                forms.update([rus, rus.lower()])
        
        return [f for f in forms if f and 1 < len(f) <= 30]
    
    def _generate_pcr_term_forms(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è –ü–¶–†-—Ç–µ—Ä–º–∏–Ω–æ–≤ (–ë–ï–ó –£–ß–ï–¢–ê –†–ï–ì–ò–°–¢–†–ê)"""
        if not text:
            return []
        
        forms = set()
        text_stripped = text.strip()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã - –¢–û–õ–¨–ö–û –ù–ò–ñ–ù–ò–ô –†–ï–ì–ò–°–¢–† –¥–ª—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–æ–∫
        forms.add(text_stripped.lower())
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (—Ç–æ–ª—å–∫–æ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', text_stripped):
            words = text_stripped.split()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –æ—Ç–¥–µ–ª—å–Ω–æ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
            processed_words = []
            for word in words:
                word_forms = {word.lower()}  # –¢–æ–ª—å–∫–æ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
                try:
                    parsed = self.morph.parse(word)[0]
                    if parsed.tag.POS in ['NOUN', 'ADJF', 'ADJS']:
                        # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Ç–æ–∂–µ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                        word_forms.add(parsed.normal_form.lower())
                        # –ü–∞–¥–µ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã —Ç–æ–∂–µ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                        for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                            try:
                                inflected = parsed.inflect({case})
                                if inflected:
                                    word_forms.add(inflected.word.lower())
                            except:
                                pass
                except:
                    word_forms.add(word.lower())
                processed_words.append(word_forms)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã —Å–ª–æ–≤ (–≤—Å–µ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
            if len(processed_words) == 1:
                forms.update(processed_words[0])
            else:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º
                from itertools import product
                for combination in product(*processed_words):
                    phrase = ' '.join(combination)
                    forms.add(phrase)  # –£–∂–µ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        
        return [f for f in forms if f and len(f) >= 2]
    
    def _find_existing_expansions(self, query: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ (—á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ)"""
        expansions = []
        pattern = r'(\b[\w\-]+\b)\s*\(([^)]+)\)'
        matches = re.finditer(pattern, query)
        
        for match in matches:
            expansions.append({
                'text': match.group(1).strip(),
                'expansion': match.group(2).strip(),
                'start': match.start(),
                'end': match.end()
            })
        
        return expansions
    
    def _is_position_used(self, start: int, end: int, used_positions: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ —É–∂–µ –ø–æ–∑–∏—Ü–∏—è"""
        for used_start, used_end in used_positions:
            if not (end <= used_start or start >= used_end):
                return True
        return False
    
    def _is_part_of_existing_expansion(self, start: int, end: int, existing_expansions: List[Dict]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        for expansion in existing_expansions:
            if start >= expansion['start'] and end <= expansion['end']:
                return True
        return False
    
    def _find_matches_in_query(self, query: str, search_dict: Dict, dict_type: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏"""
        matches = []
        words = query.split()
        
        for n in range(min(4, len(words)), 0, -1):
            for i in range(len(words) - n + 1):
                ngram = ' '.join(words[i:i + n])
                start = query.find(ngram)
                end = start + len(ngram)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                if (start < 0 or 
                    self._is_position_used(start, end, used_positions) or
                    self._is_part_of_existing_expansion(start, end, existing_expansions)):
                    continue
                
                # üîÑ –î–õ–Ø –†–ê–°–®–ò–§–†–û–í–û–ö –ò–ì–ù–û–†–ò–†–£–ï–ú –†–ï–ì–ò–°–¢–†
                if dict_type == 'pcr_full':
                    # –ò—â–µ–º –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                    ngram_lower = ngram.lower()
                    if ngram_lower in search_dict:
                        matches.append({
                            'start': start, 
                            'end': end, 
                            'found_text': ngram,  # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
                            'data': search_dict[ngram_lower],  # –î–∞–Ω–Ω—ã–µ –∏–∑ —Å–ª–æ–≤–∞—Ä—è (–ø–æ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
                            'dict_type': dict_type,
                            'word_count': n
                        })
                        used_positions.add((start, end))
                else:
                    # –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    if ngram in search_dict:
                        matches.append({
                            'start': start, 
                            'end': end, 
                            'found_text': ngram,
                            'data': search_dict[ngram],
                            'dict_type': dict_type,
                            'word_count': n
                        })
                        used_positions.add((start, end))
        
        return matches
    
    def _apply_pcr_expansions(self, query: str, matches: List[Dict]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –ü–¶–†-—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π"""
        if not matches:
            return query
        
        result = query
        offset = 0
        
        for match in matches:
            start = match['start'] + offset
            end = match['end'] + offset
            found_text = match['found_text']
            data = match['data']
            dict_type = match['dict_type']
            
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º
            expanded = found_text
            
            if dict_type == 'pcr_abbr':
                original_name = data.get('original_name', '')
                if original_name and original_name != found_text:
                    expanded = f"{found_text} ({original_name})"
                    
            elif dict_type == 'pcr_full':
                original_abbr = data.get('original_abbreviation', '')
                if original_abbr and original_abbr != found_text:
                    expanded = f"{found_text} ({original_abbr})"
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
            if expanded != found_text:
                result = result[:start] + expanded + result[end:]
                offset += len(expanded) - len(found_text)
                logger.debug(f"üîç –ü–¶–† —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{found_text}' -> '{expanded}'")
        
        return result
    
    def process_table(self, df: pd.DataFrame) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        logger.info("üîß –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É –ü–¶–†-—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π")
        
        pcr_abbr = {}
        pcr_full = {}
        
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row.get('–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                full = self._safe_string_conversion(row.get('–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞', ''))
                
                if not abbr or not full:
                    continue
                
                # üîÑ –†–ê–ó–î–ï–õ–Ø–ï–ú –í–ê–†–ò–ê–ù–¢–´ –í –†–ê–°–®–ò–§–†–û–í–ö–ê–• –ü–û /
                full_variants = self._split_variants(full, '/')
                
                # 1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –ü–¶–† (—Å —Ä–µ–≥–∏—Å—Ç—Ä–∞–º–∏)
                abbr_forms = self._generate_all_abbreviation_forms(abbr)
                for af in abbr_forms:
                    if af not in pcr_abbr:
                        pcr_abbr[af] = {
                            'original_name': full_variants[0] if full_variants else full,  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                            'original_abbr': abbr,
                            'type': 'pcr_abbr'
                        }
                
                # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ü–¶–† (–¢–û–õ–¨–ö–û –ù–ò–ñ–ù–ò–ô –†–ï–ì–ò–°–¢–†)
                for full_variant in full_variants:
                    full_forms = self._generate_pcr_term_forms(full_variant)
                    for ff in full_forms:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ —Ç–æ–ª—å–∫–æ –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                        if ff not in pcr_full:
                            pcr_full[ff] = {
                                'original_abbreviation': abbr,
                                'original_name': full_variant,
                                'type': 'pcr_full'
                            }
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –≤ –ü–¶–†: {e}")
        
        logger.info(f"‚úÖ –ü–¶–†-—Å–æ–∫—Ä–∞—â–µ–Ω–∏—è: {len(pcr_abbr)} –∞–±–±—Ä, {len(pcr_full)} –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        return {
            'pcr_abbr': pcr_abbr,
            'pcr_full': pcr_full
        }
    
    def expand_query(self, query: str, pcr_dicts: Dict) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ü–¶–†-—Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –∫ –∑–∞–ø—Ä–æ—Å—É"""
        if not query:
            return query
        
        # –ù–∞—Ö–æ–¥–∏–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ
        existing_expansions = self._find_existing_expansions(query)
        used_positions = set((exp['start'], exp['end']) for exp in existing_expansions)
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –æ–±–æ–∏—Ö —Å–ª–æ–≤–∞—Ä—è—Ö, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        all_matches = []
        all_matches.extend(self._find_matches_in_query(query, pcr_dicts['pcr_abbr'], 'pcr_abbr', used_positions, existing_expansions))
        all_matches.extend(self._find_matches_in_query(query, pcr_dicts['pcr_full'], 'pcr_full', used_positions, existing_expansions))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏) –∏ –ø–æ–∑–∏—Ü–∏–∏
        all_matches.sort(key=lambda x: (-x['word_count'], x['start']))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        return self._apply_pcr_expansions(query, all_matches)