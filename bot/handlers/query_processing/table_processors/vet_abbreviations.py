import re
import pandas as pd
from typing import Dict, List
import logging

logger = logging.getLogger(__name__)

class VetAbbreviationsProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–∞–±–ª–∏—Ü—ã –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä - –°–ê–ú –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
    
    def __init__(self, morph_analyzer):
        self.morph = morph_analyzer
    
    def _safe_string_conversion(self, value) -> str:
        if pd.isna(value) or value is None:
            return ""
        return str(value).strip()
    
    def _split_variants(self, text: str, delimiter: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é"""
        if not text:
            return []
        variants = [v.strip() for v in re.split(re.escape(delimiter), text) if v.strip()]
        return variants if variants else [text]
    
    def _generate_all_abbreviation_forms(self, abbr: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä (—Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è + —Ä–µ–≥–∏—Å—Ç—Ä—ã)"""
        if not abbr:
            return []
        
        forms = set()
        forms.update([abbr, abbr.upper(), abbr.lower(), abbr.capitalize()])
        
        # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∫–∏—Ä–∏–ª–ª–∏—Ü–∞-–ª–∞—Ç–∏–Ω–∏—Ü–∞
        rus_to_eng = {
            '–ê': 'A', '–ë': 'B', '–í': 'V', '–ì': 'G', '–î': 'D', '–ï': 'E', '–Å': 'E',
            '–ñ': 'ZH', '–ó': 'Z', '–ò': 'I', '–ô': 'Y', '–ö': 'K', '–õ': 'L', '–ú': 'M',
            '–ù': 'N', '–û': 'O', '–ü': 'P', '–†': 'R', '–°': 'S', '–¢': 'T', '–£': 'U',
            '–§': 'F', '–•': 'KH', '–¶': 'TS', '–ß': 'CH', '–®': 'SH', '–©': 'SCH',
            '–´': 'Y', '–≠': 'E', '–Æ': 'YU', '–Ø': 'YA'
        }
        
        eng_to_rus = {
            'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 'F': '–§', 'G': '–ì', 
            'H': '–•', 'I': '–ò', 'J': '–î–ñ', 'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 
            'O': '–û', 'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢', 'U': '–£', 
            'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò', 'Z': '–ó'
        }
        
        # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞
        if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', abbr):
            eng = ''.join(rus_to_eng.get(ch.upper(), ch) for ch in abbr).upper()
            if eng != abbr.upper():
                forms.update([eng, eng.lower()])
        
        # –õ–∞—Ç–∏–Ω–∏—Ü–∞ -> –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
        elif re.search(r'[A-Za-z]', abbr):
            rus = ''.join(eng_to_rus.get(ch.upper(), ch) for ch in abbr).upper()
            if rus != abbr.upper():
                forms.update([rus, rus.lower()])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—á–µ–∫
        if '.' in abbr:
            no_dots = abbr.replace('.', '')
            forms.update([no_dots, no_dots.upper(), no_dots.lower()])
        elif abbr.isupper() and 2 <= len(abbr) <= 6:
            with_dots = '.'.join(list(abbr)) + '.'
            forms.update([with_dots, with_dots.lower()])

        return [f for f in forms if f and 1 < len(f) <= 30]
    
    def _get_word_normal_form(self, word: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞"""
        if not word or not re.search(r'[–ê-–Ø–∞-—è–Å—ë]', word):
            return word.lower()
        
        try:
            parsed = self.morph.parse(word)[0]
            return parsed.normal_form
        except:
            return word.lower()
    
    def _generate_medical_term_forms(self, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        if not text:
            return []
        
        forms = set()
        text_stripped = text.strip()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –≤—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
        forms.update([
            text_stripped,
            text_stripped.lower(),
            text_stripped.upper(),
        ])
        
        # üîÑ –î–õ–Ø –ú–ù–û–ì–û–°–õ–û–í–ù–´–• –¢–ï–†–ú–ò–ù–û–í - –ì–ï–ù–ï–†–ò–†–£–ï–ú –§–û–†–ú–´ –ö–ê–ñ–î–û–ì–û –°–õ–û–í–ê
        words = text_stripped.split()
        
        if len(words) > 1:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–æ—Ä–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
            word_forms_list = []
            for word in words:
                word_forms = set()
                word_forms.add(word.lower())
                
                # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞
                normal_form = self._get_word_normal_form(word)
                word_forms.add(normal_form)
                
                # –ü–∞–¥–µ–∂–Ω—ã–µ —Ñ–æ—Ä–º—ã –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤
                if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', word):
                    try:
                        parsed = self.morph.parse(word)[0]
                        if parsed.tag.POS in ['NOUN', 'ADJF', 'ADJS']:
                            for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                                try:
                                    inflected = parsed.inflect({case})
                                    if inflected:
                                        word_forms.add(inflected.word.lower())
                                except:
                                    pass
                    except:
                        pass
                
                word_forms_list.append(list(word_forms))
            
            # üîÑ –°–û–ó–î–ê–ï–ú –í–°–ï –ö–û–ú–ë–ò–ù–ê–¶–ò–ò –§–û–†–ú –°–õ–û–í
            from itertools import product
            for combination in product(*word_forms_list):
                phrase = ' '.join(combination)
                forms.add(phrase)
        
        else:
            # –û–¥–Ω–æ —Å–ª–æ–≤–æ - –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ —Ñ–æ—Ä–º—ã
            word = words[0]
            forms.add(self._get_word_normal_form(word))
            
            if re.search(r'[–ê-–Ø–∞-—è–Å—ë]', word):
                try:
                    parsed = self.morph.parse(word)[0]
                    if parsed.tag.POS in ['NOUN', 'ADJF', 'ADJS']:
                        for case in ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']:
                            try:
                                inflected = parsed.inflect({case})
                                if inflected:
                                    forms.add(inflected.word.lower())
                            except:
                                pass
                except:
                    pass

        return [f for f in forms if f and len(f) >= 2]
    
    def _find_existing_expansions(self, query: str) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ (—á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ)"""
        expansions = []
        pattern = r'(\b[\w\-]+\b)\s*\(([^)]+)\)'
        matches = re.finditer(pattern, query)
        
        for match in matches:
            expansions.append({
                'text': match.group(1).strip(),
                'expansion': match.group(2).strip(),
                'start': match.start(),
                'end': match.end()
            })
        
        return expansions
    
    def _is_position_used(self, start: int, end: int, used_positions: set) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ —É–∂–µ –ø–æ–∑–∏—Ü–∏—è"""
        for used_start, used_end in used_positions:
            if not (end <= used_start or start >= used_end):
                return True
        return False
    
    def _is_part_of_existing_expansion(self, start: int, end: int, existing_expansions: List[Dict]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è"""
        for expansion in existing_expansions:
            if start >= expansion['start'] and end <= expansion['end']:
                return True
        return False
    
    def _find_whole_word_matches(self, query: str, search_dict: Dict, dict_type: str, used_positions: set, existing_expansions: List[Dict]) -> List[Dict]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¶–ï–õ–´–• –°–õ–û–í –≤ –∑–∞–ø—Ä–æ—Å–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è"""
        matches = []
        
        # üîÑ –ò–©–ï–ú –¢–û–õ–¨–ö–û –¶–ï–õ–´–ï –°–õ–û–í–ê –° –ì–†–ê–ù–ò–¶–ê–ú–ò –°–õ–û–í
        if dict_type == 'vet_abbr':
            # –î–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä - –∏—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
            pattern = r'\b(' + '|'.join(re.escape(key) for key in search_dict.keys()) + r')\b'
            for match in re.finditer(pattern, query, re.IGNORECASE):
                start, end = match.start(), match.end()
                found_text = match.group()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                if (self._is_position_used(start, end, used_positions) or
                    self._is_part_of_existing_expansion(start, end, existing_expansions)):
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—É—á–∏—Ç—ã–≤–∞—è —Ä–µ–≥–∏—Å—Ç—Ä –¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä)
                if found_text in search_dict:
                    matches.append({
                        'start': start, 
                        'end': end, 
                        'found_text': found_text,
                        'data': search_dict[found_text],
                        'dict_type': dict_type,
                        'word_count': 1
                    })
                    used_positions.add((start, end))
        
        else:
            # –î–ª—è –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π - –∏—â–µ–º n-gram —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
            words = query.split()
            
            for n in range(min(4, len(words)), 0, -1):
                for i in range(len(words) - n + 1):
                    ngram = ' '.join(words[i:i + n])
                    ngram_lower = ngram.lower()
                    
                    # üîÑ –ò–©–ï–ú –° –ì–†–ê–ù–ò–¶–ê–ú–ò –°–õ–û–í
                    pattern = r'\b' + re.escape(ngram) + r'\b'
                    match = re.search(pattern, query, re.IGNORECASE)
                    
                    if not match:
                        continue
                        
                    start, end = match.start(), match.end()
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–ª–∏ –≤–Ω—É—Ç—Ä–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                    if (self._is_position_used(start, end, used_positions) or
                        self._is_part_of_existing_expansion(start, end, existing_expansions)):
                        continue
                    
                    # –î–ª—è –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∏—â–µ–º –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                    if ngram_lower in search_dict:
                        matches.append({
                            'start': start, 
                            'end': end, 
                            'found_text': ngram,
                            'data': search_dict[ngram_lower],
                            'dict_type': dict_type,
                            'word_count': n
                        })
                        used_positions.add((start, end))
        
        return matches
    
    def _apply_vet_expansions(self, query: str, matches: List[Dict]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
        if not matches:
            return query
        
        result = query
        offset = 0
        
        for match in matches:
            start = match['start'] + offset
            end = match['end'] + offset
            found_text = match['found_text']
            data = match['data']
            dict_type = match['dict_type']
            
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º
            expanded = found_text
            
            if dict_type == 'vet_abbr':
                original_names = data.get('original_names', [])
                if original_names and original_names[0] != found_text:
                    expanded = f"{found_text} ({original_names[0]})"
                    
            elif dict_type == 'vet_full':
                original_abbrs = data.get('original_abbreviations', [])
                if original_abbrs and original_abbrs[0] != found_text:
                    expanded = f"{found_text} ({original_abbrs[0]})"
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ –æ–Ω–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å
            if expanded != found_text:
                result = result[:start] + expanded + result[end:]
                offset += len(expanded) - len(found_text)
                logger.debug(f"üîç –í–µ—Ç. —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ: '{found_text}' -> '{expanded}'")
        
        return result
    
    def process_table(self, df: pd.DataFrame) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        logger.info("üîß –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä")
        
        vet_abbr = {}
        vet_full = {}
        
        for _, row in df.iterrows():
            try:
                abbr = self._safe_string_conversion(row.get('–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞', ''))
                rus_name = self._safe_string_conversion(row.get('–†—É—Å—Å–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/—Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞', ''))
                
                if not abbr or not rus_name:
                    continue
                
                # üîÑ –†–ê–ó–î–ï–õ–Ø–ï–ú –í–ê–†–ò–ê–ù–¢–´ –ü–û /
                abbr_variants = self._split_variants(abbr, '/')
                rus_variants = self._split_variants(rus_name, '/')
                
                # 1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                for a in abbr_variants:
                    abbr_forms = self._generate_all_abbreviation_forms(a)
                    for af in abbr_forms:
                        if af not in vet_abbr:
                            vet_abbr[af] = {
                                'original_names': rus_variants,
                                'original_abbr': a,
                                'type': 'vet_abbr'
                            }
                
                # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä—É—Å—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–° –í–ê–†–ò–ê–¶–ò–Ø–ú–ò –ö–ê–ñ–î–û–ì–û –°–õ–û–í–ê)
                for r in rus_variants:
                    full_forms = self._generate_medical_term_forms(r)
                    for ff in full_forms:
                        # üîÑ –°–û–•–†–ê–ù–Ø–ï–ú –í –ù–ò–ñ–ù–ï–ú –†–ï–ì–ò–°–¢–†–ï –î–õ–Ø –ü–û–ò–°–ö–ê
                        ff_lower = ff.lower()
                        if ff_lower not in vet_full:
                            vet_full[ff_lower] = {
                                'original_abbreviations': abbr_variants,
                                'original_name': r,
                                'type': 'vet_full'
                            }
                            
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ç. –∞–±–±—Ä.: {e}")
        
        # üîÑ –§–ò–õ–¨–¢–†–£–ï–ú –ö–û–†–û–¢–ö–ò–ï –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–´, –ö–û–¢–û–†–´–ï –ú–û–ì–£–¢ –ë–´–¢–¨ –ß–ê–°–¢–¨–Æ –°–õ–û–í
        problematic_abbrs = ['na', '–Ω–∞', 'ab', 'ag', 'ca', 'fe', 'k', 'p']  # –ß–∞—Å—Ç–∏ —Å–ª–æ–≤
        for problem_abbr in problematic_abbrs:
            if problem_abbr in vet_abbr:
                logger.debug(f"‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É: '{problem_abbr}'")
                del vet_abbr[problem_abbr]
            if problem_abbr.upper() in vet_abbr:
                logger.debug(f"‚ö†Ô∏è –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—É—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É: '{problem_abbr.upper()}'")
                del vet_abbr[problem_abbr.upper()]
        
        logger.info(f"‚úÖ –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã: {len(vet_abbr)} –∞–±–±—Ä, {len(vet_full)} –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        return {
            'vet_abbr': vet_abbr,
            'vet_full': vet_full
        }
    
    def expand_query(self, query: str, vet_dicts: Dict) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –∫ –∑–∞–ø—Ä–æ—Å—É"""
        if not query:
            return query
        
        # –ù–∞—Ö–æ–¥–∏–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –≤ –∑–∞–ø—Ä–æ—Å–µ
        existing_expansions = self._find_existing_expansions(query)
        used_positions = set((exp['start'], exp['end']) for exp in existing_expansions)
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –æ–±–æ–∏—Ö —Å–ª–æ–≤–∞—Ä—è—Ö, –∏–≥–Ω–æ—Ä–∏—Ä—É—è —É–∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
        all_matches = []
        all_matches.extend(self._find_whole_word_matches(query, vet_dicts['vet_abbr'], 'vet_abbr', used_positions, existing_expansions))
        all_matches.extend(self._find_whole_word_matches(query, vet_dicts['vet_full'], 'vet_full', used_positions, existing_expansions))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏) –∏ –ø–æ–∑–∏—Ü–∏–∏
        all_matches.sort(key=lambda x: (-x['word_count'], x['start']))
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        result = self._apply_vet_expansions(query, all_matches)
        
        if result != query:
            logger.info(f"üîç –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω—ã: '{query}' -> '{result}'")
        
        return result