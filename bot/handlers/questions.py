from aiogram import Router, F
from aiogram.types import Message
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from bot.keyboards import get_cancel_kb, get_menu_by_role
from src.database.db_init import db
from src.data_vectorization import DataProcessor
from models.models_init import gemma3_27b_instruct_free as llm
from langchain.schema import SystemMessage, HumanMessage
import json

questions_router = Router()

class QuestionStates(StatesGroup):
    waiting_for_question = State()

@questions_router.message(F.text.in_(["‚ùì –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å", "ü§ñ –í–æ–ø—Ä–æ—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"]))
async def start_question(message: Message, state: FSMContext):
    """Begin question flow and reset ephemeral memory buffer."""
    user_id = message.from_user.id
    print(f"[INFO] User {user_id} initiated question flow")

    user = await db.get_user(user_id)
    if not user:
        print(f"[WARN] User {user_id} not registered")
        await message.answer(
            "–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–æ–π—Ç–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é.\n"
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É /start"
        )
        return

    # determine role, default to 'client' if missing
    role = user['role'] if 'role' in user.keys() else 'client'
    print(f"[INFO] Resolved role for user {user_id}: {role}")

    # clear only the short-term buffer, keep earlier summary
    await db.clear_buffer(user_id)

    # choose prompt by role
    if role == 'staff':
        prompt = (
            "ü§ñ –ó–∞–¥–∞–π—Ç–µ –≤–∞—à –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.\n\n"
            "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å:\n"
            "‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ª–µ—á–µ–Ω–∏–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö\n"
            "‚Ä¢ –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è–º–∏ –ø–æ —Å–∏–º–ø—Ç–æ–º–∞–º\n"
            "‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —É—Ö–æ–¥—É\n"
            "‚Ä¢ –í–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏"
        )
    else:
        prompt = (
            "‚ùì –ó–∞–¥–∞–π—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å.\n\n"
            "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å:\n"
            "‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–ª–∏–Ω–∏–∫–µ\n"
            "‚Ä¢ –£—Å–ª—É–≥–∞–º–∏ –∏ —Ü–µ–Ω–∞–º–∏\n"
            "‚Ä¢ –†–µ–∂–∏–º–æ–º —Ä–∞–±–æ—Ç—ã\n"
            "‚Ä¢ –û–±—â–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –æ –ø–∏—Ç–æ–º—Ü–∞—Ö"
        )

    await message.answer(prompt, reply_markup=get_cancel_kb())
    await state.set_state(QuestionStates.waiting_for_question)
    print(f"[INFO] State set to waiting_for_question for user {user_id}")

@questions_router.message(QuestionStates.waiting_for_question)
async def process_question(message: Message, state: FSMContext):
    """Handle user question: update memory, fetch RAG context, ask LLM, update memory."""
    user_id = message.from_user.id
    text = message.text.strip()

    if text == "‚ùå –û—Ç–º–µ–Ω–∞":
        await state.clear()
        user = await db.get_user(user_id)
        role = user['role'] if 'role' in user.keys() else 'client'
        await message.answer("–û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞.", reply_markup=get_menu_by_role(role))
        print(f"[INFO] User {user_id} cancelled question")
        return

    # resolve role again
    user = await db.get_user(user_id)
    role = user['role'] if 'role' in user.keys() else 'client'
    print(f"[INFO] User {user_id} submitted question: {text} (role={role})")

    # 1) Log and buffer the user question
    await db.add_request_stat(user_id, "question", text[:200])
    await db.add_memory(user_id, 'buffer', f"User: {text}")
    print(f"[INFO] Question logged and buffered for user {user_id}")

    # 2) Retrieve memory summary + buffer
    summary = await db.get_latest_summary(user_id) or ""
    buffer = await db.get_buffer(user_id)
    print(f"[INFO] Retrieved summary and buffer for user {user_id}")

    # 3) Retrieve RAG context
    processor = DataProcessor()
    processor.load_vector_store()
    rag_hits = processor.search_test(text, top_k=5)
    print(f"[INFO] Retrieved {len(rag_hits)} RAG hits for user {user_id}")

    # 4) Build prompt sections
    memory_section = ""
    if summary:
        memory_section += f"–°–≤–æ–¥–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π: {summary}\n\n"
    if buffer:
        memory_section += "–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n" + "\n".join(buffer) + "\n\n"

    rag_blocks = []
    for doc, score in rag_hits:
        meta_json = json.dumps(doc.metadata, ensure_ascii=False, sort_keys=True)
        rag_blocks.append(f"–¢–µ—Å—Ç: {doc.page_content}\n–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {meta_json}")
    rag_context = "\n\n---\n\n".join(rag_blocks)

    system_msg = SystemMessage(
        content="–¢—ã ‚Äî –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π –ø–∞–º—è—Ç—å –∏ –ø—Ä–µ–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞."
    )
    user_msg = HumanMessage(
        content=(
            f"{memory_section}"
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–∞–Ω–∞–ª–∏—Ç–∏–∫–∏:\n{rag_context}\n\n"
            f"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {text}\n\n"
            "–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É."
        )
    )

    # 5) Call LLM
    print(f"[INFO] Sending prompt to LLM for user {user_id}")
    response = await llm.agenerate([[system_msg, user_msg]])
    answer = response.generations[0][0].text.strip()
    print(f"[INFO] Received LLM answer for user {user_id}")

    # 6) Buffer the bot response
    await db.add_memory(user_id, 'buffer', f"Bot: {answer}")
    print(f"[INFO] Bot response buffered for user {user_id}")

    # 7) Send answer back
    await message.answer(answer, reply_markup=get_menu_by_role(role))
    print(f"[INFO] Answer sent to user {user_id}")

    await state.clear()
    print(f"[INFO] State cleared for user {user_id}")
