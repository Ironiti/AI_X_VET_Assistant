from aiogram import Router, F
from aiogram.types import Message
import base64
import html
import re
import pandas as pd
import re
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import unicodedata
import string
from aiogram.filters import Command

BOT_USERNAME = "AL_VET_UNION_BOT"

gif_router = Router()
file_router = Router()

@file_router.message(F.document)
async def handle_document(message: Message):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    document = message.document
    
    file_id = document.file_id
    file_name = document.file_name or "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
    file_size = document.file_size or 0
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = (
        f"üìÑ <b>–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ:</b>\n"
        f"üìÅ –ò–º—è: {html.escape(file_name)}\n"
        f"üìè –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç\n"
        f"üÜî File ID: <code>{file_id}</code>\n\n"
    )
    
    await message.answer(response, parse_mode="HTML")

@gif_router.message(F.animation)
async def get_gif_id(message: Message):
    await message.answer(f"GIF ID: {message.animation.file_id}")


def fix_bold(text: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç markdown –∂–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ HTML."""

    # –ó–∞–º–µ–Ω—è–µ–º **—Ç–µ–∫—Å—Ç** –Ω–∞ <b>—Ç–µ–∫—Å—Ç</b>
    text = re.sub(r"\*\*([^\*]+)\*\*", r"<b>\1</b>", text)
    return text


def safe_html(text: str) -> str:
    """–≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç HTML —Å–∏–º–≤–æ–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
    return html.escape(text)


def normalize_test_code(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤–≤–µ–¥–µ–Ω–Ω—ã–π –∫–æ–¥ —Ç–µ—Å—Ç–∞."""
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ None –∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
    if not text:
        return ""

    text = text.strip().upper().replace(" ", "")

    # –ó–∞–º–µ–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É –Ω–∞ –ª–∞—Ç–∏–Ω–∏—Ü—É –≤ –ø—Ä–µ—Ñ–∏–∫—Å–µ AN/–ê–ù
    text = text.replace("–ê–ù", "AN").replace("–êN", "AN").replace("A–ù", "AN")

    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã - –¥–æ–±–∞–≤–ª—è–µ–º AN
    if text.isdigit():
        text = f"AN{text}"
    # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ü–∏—Ñ—Ä - –¥–æ–±–∞–≤–ª—è–µ–º AN –≤ –Ω–∞—á–∞–ª–æ
    elif re.match(r"^\d", text):
        text = f"AN{text}"

    return text


def encode_test_code_for_url(test_code: str) -> str:
    """–ù–∞–¥–µ–∂–Ω–æ –∫–æ–¥–∏—Ä—É–µ—Ç –∫–æ–¥ —Ç–µ—Å—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ URL."""
    if not test_code:
        return ""

    try:
        # –ö–æ–¥–∏—Ä—É–µ–º –≤ bytes, –∑–∞—Ç–µ–º –≤ URL-safe base64
        encoded_bytes = test_code.encode("utf-8")
        encoded_b64 = base64.urlsafe_b64encode(encoded_bytes).decode("ascii")
        # –£–±–∏—Ä–∞–µ–º padding '=' –¥–ª—è –±–æ–ª–µ–µ —á–∏—Å—Ç—ã—Ö URL
        encoded_b64 = encoded_b64.rstrip("=")
        return encoded_b64
    except Exception as e:
        print(f"[ERROR] Failed to encode test code '{test_code}': {e}")
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ ASCII —Å–∏–º–≤–æ–ª—ã
        safe_code = "".join(c if c.isascii() else "_" for c in test_code)
        return base64.urlsafe_b64encode(safe_code.encode()).decode().rstrip("=")


def decode_test_code_from_url(encoded_code: str) -> str:
    """–ù–∞–¥–µ–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç –∫–æ–¥ —Ç–µ—Å—Ç–∞ –∏–∑ URL."""
    if not encoded_code:
        return ""

    try:
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º padding –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        padding_needed = 4 - (len(encoded_code) % 4)
        if padding_needed and padding_needed != 4:
            encoded_code += "=" * padding_needed

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
        decoded_bytes = base64.urlsafe_b64decode(encoded_code)
        decoded_str = decoded_bytes.decode("utf-8")

        print(f"[DEBUG] Successfully decoded: {encoded_code} -> {decoded_str}")
        return decoded_str

    except Exception as e:
        print(f"[ERROR] Failed to decode '{encoded_code}': {e}")
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ –ø–æ–∏—Å–∫–∞
        return encoded_code


def create_test_link(test_code: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç deep link –¥–ª—è —Ç–µ—Å—Ç–∞."""
    
    if "AN520" in test_code and "," in test_code:
        test_code = "AN520–ì–ò–≠"
    
    if not test_code:
        return f"https://t.me/{BOT_USERNAME}"

    encoded_code = encode_test_code_for_url(test_code)
    link = f"https://t.me/{BOT_USERNAME}?start=test_{encoded_code}"

    return link


def split_long_message(text: str, max_length: int = 4000) -> list[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏"""
    if len(text) <= max_length:
        return [text]

    parts = []
    current_part = ""

    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
    lines = text.split("\n")

    for line in lines:
        if len(current_part) + len(line) + 1 > max_length:
            if current_part:
                parts.append(current_part.strip())
                current_part = line
            else:
                # –ï—Å–ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è, —Ä–µ–∂–µ–º –µ—ë
                while len(line) > max_length:
                    parts.append(line[:max_length])
                    line = line[max_length:]
                current_part = line
        else:
            current_part += "\n" + line if current_part else line

    if current_part:
        parts.append(current_part.strip())

    return parts


async def safe_delete_message(message):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    try:
        if message:
            await message.delete()
    except Exception:
        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —É–¥–∞–ª–µ–Ω–∏—è


def is_test_code_pattern(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—É –∫–æ–¥–∞ —Ç–µ—Å—Ç–∞."""
    text = text.strip().upper().replace(" ", "")

    patterns = [
        r"^[A–ê][N–ù]\d+",  # AN –∏–ª–∏ –ê–ù + —Ü–∏—Ñ—Ä—ã
        r"^[A–ê][N–ù]\d+[A-Z–ê-–Ø\-]+",  # AN + —Ü–∏—Ñ—Ä—ã + —Å—É—Ñ—Ñ–∏–∫—Å
        # –°—Ç—Ä–æ–∂–µ: —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–¥—ã –¥–ª–∏–Ω–æ–π 2-4, –Ω–µ 1 —Å–∏–º–≤–æ–ª
        r"^\d{2,4}$",
        r"^\d+[A-Z–ê-–Ø\-]+$",  # –¶–∏—Ñ—Ä—ã + –±—É–∫–≤–µ–Ω–Ω—ã–π —Å—É—Ñ—Ñ–∏–∫—Å
    ]

    for pattern in patterns:
        if re.match(pattern, text):
            return True

    return False

def normalize_text(text: str) -> str:
    if not text or pd.isna(text):
        return ""
    
    text = str(text).strip()
    
    if not text:
        return ""
    
    text = unicodedata.normalize('NFKD', text)
    text = text.replace('—ë', '–µ').replace('–Å', '–µ')
    
    text = text.lower()
    
    text = re.sub(r'[^\w\s,-]', ' ', text)
    
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'(\s*,\s*)', ', ', text)  # –∑–∞–ø—è—Ç—ã–µ
    text = text.strip()
    
    return text


def transliterate_abbreviation(abbr: str) -> Optional[str]:

    # –°–ª–æ–≤–∞—Ä—å —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏
    translit_dict = {
        'A': '–ê', 'B': '–í', 'C': '–°', 'D': '–î', 'E': '–ï', 
        'F': '–§', 'G': '–ì', 'H': '–•', 'I': '–ò', 'J': '–î–ñ',
        'K': '–ö', 'L': '–õ', 'M': '–ú', 'N': '–ù', 'O': '–û',
        'P': '–ü', 'Q': '–ö', 'R': '–†', 'S': '–°', 'T': '–¢',
        'U': '–£', 'V': '–í', 'W': '–í', 'X': '–ö–°', 'Y': '–ò',
        'Z': '–ó'
    }
    
    # –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–ª—É—á–∏—Ç—å—Å—è
    common_russian_words = {
        '–æ—Ç', '–¥–æ', '–ø–æ', '–Ω–∞', '–∑–∞', '–∏–∑', '—Å', '—É', '–≤', '–∫',
        '–Ω–æ', '–¥–∞', '–Ω–µ—Ç', '–∞–≥–∞', '–æ–π', '–∞—Ö', '—ç—Ö', '–Ω—É', '–≤–æ—Ç',
        '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–∫–∞–∫', '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '–≥–¥–µ',
        '–∫—Ç–æ', '—á—Ç–æ', '–ø–æ—á–µ–º—É', '–∫–æ–≥–¥–∞', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–∑–∞—á–µ–º',
        '–∏', '–∞', '–∏–ª–∏', '–Ω–æ', '–∂–µ', '–±—ã', '–ª–∏', '—Ç–æ', '–Ω–∏', '–Ω–µ',
        '–¥–∞–∂–µ', '—É–∂–µ', '–µ—â–µ', '–≤—Å–µ', '–≤—Å—ë', '–≤—Å–µ–≥–æ', '–≤—Å–µ–≥–¥–∞',
        '–æ—á–µ–Ω—å', '–ø–æ—á—Ç–∏', '—Å–æ–≤—Å–µ–º', '–≤–¥—Ä—É–≥', '–ø–æ—Ç–æ–º', '—Ç–µ–ø–µ—Ä—å',
        '—Ç–æ–≥–¥–∞', '–∏–Ω–æ–≥–¥–∞', '–Ω–∏–∫–æ–≥–¥–∞', '–≤—Å–µ–≥–¥–∞', '–≤–µ–∑–¥–µ', '–Ω–∏–≥–¥–µ',
        '–Ω–∏–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–∫–∞–∫', '—Ç–∞–∫',
        '—Ç–∞–º', '—Ç—É—Ç', '–∑–¥–µ—Å—å', '–≤–æ–Ω', '—Ç—É–¥–∞', '—Å—é–¥–∞', '–æ—Ç—Ç—É–¥–∞',
        '–æ—Ç—Å—é–¥–∞', '–≤–ø–µ—Ä–µ–¥', '–Ω–∞–∑–∞–¥', '–≤–≤–µ—Ä—Ö', '–≤–Ω–∏–∑', '–≤–Ω—É—Ç—Ä—å',
        '–Ω–∞—Ä—É–∂—É', '–≤–æ–∫—Ä—É–≥', '–æ–∫–æ–ª–æ', '–≤–æ–∑–ª–µ', '–±–ª–∏–∑–∫–æ', '–¥–∞–ª–µ–∫–æ',
        '–≤—ã—Å–æ–∫–æ', '–Ω–∏–∑–∫–æ', '–≥–ª—É–±–æ–∫–æ', '–º–µ–ª–∫–æ', '—à–∏—Ä–æ–∫–æ', '—É–∑–∫–æ',
        '–¥–æ–ª–≥–æ', '—Å–∫–æ—Ä–æ', '—Ä–∞–Ω–æ', '–ø–æ–∑–¥–Ω–æ', '—á–∞—Å—Ç–æ', '—Ä–µ–¥–∫–æ',
        '–º–Ω–æ–≥–æ', '–º–∞–ª–æ', '–Ω–µ–º–Ω–æ–≥–æ', '—Å–æ–≤—Å–µ–º', '–ø–æ—á—Ç–∏', '—á—É—Ç—å',
        '–µ–ª–µ', '–µ–¥–≤–∞', '—á—É—Ç—å-—á—É—Ç—å', '–Ω–µ–º–Ω–æ–∂–∫–æ', '–Ω–µ–º–Ω–æ–≥–æ'
    }
    
    result = []
    for char in abbr.upper():
        if char in translit_dict:
            result.append(translit_dict[char])
        else:
            result.append(char)
    
    transliterated = ''.join(result)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ä—É—Å—Å–∫–∏–º —Å–ª–æ–≤–æ–º
    if transliterated.lower() in common_russian_words:
        return None
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (1-2 —Å–∏–º–≤–æ–ª–∞)
    if len(transliterated) <= 2:
        return None
    
    return transliterated

def normalize_container_name(container_name: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    
    Args:
        container_name: –ò—Å—Ö–æ–¥–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
        
    Returns:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    """
    if not container_name or not isinstance(container_name, str):
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫
    normalized = " ".join(container_name.strip().split())
    
    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
    normalized = normalized.replace('"', "").replace("'", "")
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    normalized_lower = normalized.lower()
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±—â–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
    # –ó–∞–º–µ–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Ü–≤–µ—Ç–æ–≤
    color_variations = {
        r'\b(—Å\s+)?—Å–∏—Ä–µ–Ω–µ–≤–æ–π\b': '—Å —Å–∏—Ä–µ–Ω–µ–≤–æ–π',
        r'\b(—Å\s+)?—Ñ–∏–æ–ª–µ—Ç–æ–≤–æ–π\b': '—Å —Å–∏—Ä–µ–Ω–µ–≤–æ–π',  # —Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π = —Å–∏—Ä–µ–Ω–µ–≤—ã–π
        r'\b(—Å\s+)?–∫—Ä–∞—Å–Ω–æ–π\b': '—Å –∫—Ä–∞—Å–Ω–æ–π',
        r'\b(—Å\s+)?—Å–∏–Ω–µ–π\b': '—Å —Å–∏–Ω–µ–π',
        r'\b(—Å\s+)?–∑–µ–ª–µ–Ω–æ–π\b': '—Å –∑–µ–ª–µ–Ω–æ–π',
        r'\b(—Å\s+)?–∂–µ–ª—Ç–æ–π\b': '—Å –∂–µ–ª—Ç–æ–π',
        r'\b(—Å\s+)?–±–µ–ª–æ–π\b': '—Å –±–µ–ª–æ–π',
        r'\b(—Å\s+)?—Å–µ—Ä–æ–π\b': '—Å —Å–µ—Ä–æ–π',
    }
    
    for pattern, replacement in color_variations.items():
        normalized_lower = re.sub(pattern, replacement, normalized_lower, flags=re.IGNORECASE)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è "–∫—Ä—ã—à–∫–∞/–∫—Ä—ã—à–∫–æ–π"
    normalized_lower = re.sub(r'\b–∫—Ä—ã—à–∫[–∞–æ–π]\b', '–∫—Ä—ã—à–∫–æ–π', normalized_lower)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º "–ø—Ä–æ–±–∏—Ä–∫–∞"
    normalized_lower = re.sub(r'\b–ø—Ä–æ–±–∏—Ä–∫[–∞–µ–∏]\b', '–ø—Ä–æ–±–∏—Ä–∫–∞', normalized_lower)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    normalized_lower = " ".join(normalized_lower.split())
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º (–ø–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è)
    return " ".join(word.capitalize() for word in normalized_lower.split())


def deduplicate_container_names(container_names: List[str]) -> List[str]:
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤.
    
    Args:
        container_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
    """
    if not container_names:
        return []
    
    seen_normalized = set()
    unique_containers = []
    
    for container in container_names:
        if not container or not isinstance(container, str):
            continue
            
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        normalized = normalize_container_name(container)
        normalized_key = normalized.lower()
        
        # –ï—Å–ª–∏ —Ç–∞–∫–æ–≥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –µ—â–µ –Ω–µ –±—ã–ª–æ
        if normalized_key not in seen_normalized and normalized_key.strip():
            seen_normalized.add(normalized_key)
            unique_containers.append(normalized)
    
    return unique_containers

def is_profile_test(test_code: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Å—Ç –ø—Ä–æ—Ñ–∏–ª–µ–º (–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –û–ë–°)"""
    if not test_code:
        return False
    return test_code.upper().strip().endswith("–û–ë–°")

def check_profile_request(query: str) -> tuple[bool, str]:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Ñ–∏–ª–∏ –∏ –æ—á–∏—â–∞–µ—Ç –∑–∞–ø—Ä–æ—Å
    
    Returns:
        (is_profile_request, cleaned_query)
    """
    query_lower = query.lower()
    
    profile_keywords = [
        "–ø—Ä–æ—Ñ–∏–ª—å",
        "–ø—Ä–æ—Ñ–∏–ª–∏", 
        "–ø—Ä–æ—Ñ–∏–ª",
    ]
    
    is_profile = any(keyword in query_lower for keyword in profile_keywords)
    
    # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—Ä–æ—Ñ–∏–ª–µ–π
    cleaned_query = query
    if is_profile:
        for keyword in profile_keywords:
            cleaned_query = re.sub(rf'\b{keyword}\w*\b', '', cleaned_query, flags=re.IGNORECASE)
        cleaned_query = ' '.join(cleaned_query.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    
    return is_profile, cleaned_query.strip()


def filter_results_by_type(results: List[Tuple], show_profiles: bool = False) -> List[Tuple]:
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ç–∏–ø—É (–ø—Ä–æ—Ñ–∏–ª–∏ –∏–ª–∏ –æ–±—ã—á–Ω—ã–µ —Ç–µ—Å—Ç—ã)
    """
    # –î–û–ë–ê–í–¨–¢–ï –î–ò–ê–ì–ù–û–°–¢–ò–ö–£
    print(f"\n[FILTER DEBUG] Input: {len(results)} results, show_profiles={show_profiles}")
    
    filtered = []
    for item in results:
        doc = item[0] if isinstance(item, tuple) else item
        test_code = doc.metadata.get("test_code", "")
        
        is_profile = is_profile_test(test_code)
        
        # –î–û–ë–ê–í–¨–¢–ï –î–ò–ê–ì–ù–û–°–¢–ò–ö–£ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö
        if len(filtered) < 3 or (is_profile and show_profiles):
            print(f"  - Code: {test_code}, is_profile: {is_profile}, will_include: {show_profiles == is_profile}")
        
        if show_profiles == is_profile:
            filtered.append(item)
    
    print(f"[FILTER DEBUG] Output: {len(filtered)} results after filtering")
    return filtered
