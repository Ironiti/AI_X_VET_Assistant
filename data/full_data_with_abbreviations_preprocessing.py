import pandas as pd
import re
from bot.handlers.query_processing.vet_abbreviations_expander import vet_abbr_manager

ADDITIONAL_FILES_ID = {
    "–ë–ï–®–ï–ù–°–¢–í–û_–ü—Ä–µ–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ_—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è_–∫_—Ç–µ—Å—Ç—É_AN239RAB_–∏_AN239RABCT": 
    "BQACAgIAAxkBAAII72jQbFAOtMFdnBhD7VZFuUeLJ0HXAALBjwACmcaBSrzizdMGLOuKNgQ",
}

GENERAL_FILES_ID = {
    "–ì–µ–Ω–µ—Ç–∏–∫–∞": "BQACAgIAAxkBAAII82jQbUoZIYPP3VQ8nQ1CPZYsrEXKAALHjwACmcaBSqn9S_LmpRcvNgQ",
    "–î–µ—Ä–º–∞—Ç–æ–≥–∏—Å—Ç–æ–ø–∞—Ç–æ–ª–æ–≥–∏—è": "BQACAgIAAxkBAAII9WjQbWAiy5aknLJcs75rew0S4QqvAALIjwACmcaBSusQGqVqkwLFNgQ",
    "–ò–ì–•": "BQACAgIAAxkBAAII92jQbXOaPqCPr3V27hRAd0n8v9IUAALJjwACmcaBSjdrVPg51hNtNgQ",
    "–ö–æ—à–∫–∏ –ü–¶–†": "BQACAgIAAxkBAAII-WjQbYVetGAv-WqhWfRHtTHoCZdqAALKjwACmcaBSvaDUOqZvexnNgQ",
    "–ö–†–°": "BQACAgIAAxkBAAII-2jQbZTXE-fCWLaMtEMkCMqasKoAA8uPAAKZxoFKAAGO24UFApqBNgQ",
    "–ö—É—Ä–∏—Ü—ã": "BQACAgIAAxkBAAII_WjQbZ_ETi0J5APGfNmeF8YNcLPOAALMjwACmcaBSsya_1-lQKMUNgQ",
    "–õ–æ—à–∞–¥–∏": "BQACAgIAAxkBAAII_2jQba5DDh66bacHOCirl-S2CHnJAALOjwACmcaBSpdf1cpcrIDXNgQ",
    "–ú–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è": "BQACAgIAAxkBAAIJAAFo0G2uEJsU7aLi0tb365Zim8DyCwACzY8AApnGgUodmoxym_SdUDYE",
    "–ú–æ—Ä—Å–∫–∏–µ –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏–µ": "BQACAgIAAxkBAAIJAWjQba6Oy1jZWN-y3cR-3J9xxuNoAALPjwACmcaBSskM-hFm2_y6NgQ",
    "–ú–†–°": "BQACAgIAAxkBAAIJAmjQba4_-_aL-roU0Hj6dtEFZNywAALQjwACmcaBSiP-NhffKv7CNgQ",
    "–û–±—â–∏–π": "BQACAgIAAxkBAAIJA2jQba67tA7Ep821cfFKOTJWrtMqAALSjwACmcaBSskWqVBUyuhWNgQ",
    "–ü–∞—Ç–æ–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è": "BQACAgIAAxkBAAIJBGjQba50bmZnkcaual_HZ6i5xzjzAALRjwACmcaBSuSohFck834gNgQ",
    "–°–≤–∏–Ω—å–∏": "BQACAgIAAxkBAAIJBWjQba60VwAB3hNtgXoy9_yvO1TU6gAC1I8AApnGgUr-3nki5PqljTYE",
    "–°–æ–±–∞–∫–∏ –ü–¶–†": "BQACAgIAAxkBAAIJBmjQba5-qPCjySOWfqTKh-YGIcNaAALTjwACmcaBSmLVE-maQHEHNgQ"
}

ADDITIONAL_FILES_LINKS = {
    "–ë–ï–®–ï–ù–°–¢–í–û_–ü—Ä–µ–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ_—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è_–∫_—Ç–µ—Å—Ç—É_AN239RAB_–∏_AN239RABCT": 
    "https://disk.yandex.ru/i/Thne1AF9yvmWlg",
}

GENERAL_FILES_LINkS = {
    "–ì–µ–Ω–µ—Ç–∏–∫–∞": "https://disk.yandex.ru/i/3TTZLfvA6E1mCw",
    "–î–µ—Ä–º–∞—Ç–æ–≥–∏—Å—Ç–æ–ø–∞—Ç–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/ijzv_KTWpQgIYA",
    "–ò–ì–•": "https://disk.yandex.ru/i/qOcf6AX-_HZuzA",
    "–ö–æ—à–∫–∏ –ü–¶–†": "https://disk.yandex.ru/i/Cb5u3CbVLg8hOQ",
    "–ö–†–°": "https://disk.yandex.ru/i/JOSmA7mFIhGTrA",
    "–ö—É—Ä–∏—Ü—ã": "https://disk.yandex.ru/i/EbhWuaW2-R6OmA",
    "–õ–æ—à–∞–¥–∏": "https://disk.yandex.ru/i/zmCgRdeZw9HHqA",
    "–ú–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/DSUU8h0X2o3lJw",
    "–ú–æ—Ä—Å–∫–∏–µ –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏–µ": "https://disk.yandex.ru/i/jRDFgOiIFIrpVA",
    "–ú–†–°": "https://disk.yandex.ru/i/mVMAnU0-76lACQ",
    "–û–±—â–∏–π": "https://disk.yandex.ru/i/2ZpfPdJx5dOatQ",
    "–ü–∞—Ç–æ–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/xXm15FsDHwPhpg",
    "–°–≤–∏–Ω—å–∏": "https://disk.yandex.ru/i/R-IofXZ0qpwLSA",
    "–°–æ–±–∞–∫–∏ –ü–¶–†": "https://disk.yandex.ru/i/ZXrs4sNjduf4OQ"
}

def safe_str(value, default=""):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É, —É–±–∏—Ä–∞—è nan"""
    if pd.isna(value) or value is None or str(value).lower() in ['nan', 'none', 'null', '']:
        return ''
    return str(value).strip()

def join_pcr_data(main_file_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
    
    # –ß–∏—Ç–∞–µ–º –æ–±–∞ –ª–∏—Å—Ç–∞ –∏–∑ Excel —Ñ–∞–π–ª–∞
    main_df = pd.read_excel(main_file_path, sheet_name='–û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞')
    pcr_df = pd.read_excel(main_file_path, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†')
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±—É–∫–≤–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –∏–∑ –∫–æ–¥–∞ —Ç–µ—Å—Ç–∞
    def extract_letters(code):
        code_str = safe_str(code)
        if not code_str:
            return None
        match = re.search(r'(\d+)([A-Za-z–ê-–Ø–∞-—è]+)$', code_str)
        if match:
            return match.group(2).strip()
        return None
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –±—É–∫–≤–µ–Ω–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∫–æ–¥–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    main_df['code_letters'] = main_df['test_code'].apply(extract_letters)
    main_df['code_letters'] = main_df['code_letters'].apply(lambda x: safe_str(x).upper())
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –±—É–∫–≤–µ–Ω–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∫–æ–¥–æ–≤ –¥–ª—è —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –ü–¶–†
    pcr_df.rename(columns={'–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞': 'code_letters'}, inplace=True)
    pcr_df['code_letters'] = pcr_df['code_letters'].apply(lambda x: safe_str(x).strip())
    pcr_df['–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'] = pcr_df['–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'].apply(lambda x: ' '.join(safe_str(x).split("/")))
                                                                                     
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –ø–æ –±—É–∫–≤–µ–Ω–Ω—ã–º —á–∞—Å—Ç—è–º –∫–æ–¥–æ–≤
    merged_df = pd.merge(main_df, pcr_df, 
                        left_on='code_letters', 
                        right_on='code_letters', 
                        how='left')

    merged_df.rename(columns={'–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞': 'encoded'}, inplace=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
    merged_df['form_link'] = merged_df['form_name']\
        .apply(lambda x: safe_str(x))\
        .apply(lambda x: string_to_ids(x, GENERAL_FILES_LINkS))

    merged_df['additional_information_link'] = merged_df['additional_information_name']\
        .apply(lambda x: safe_str(x))\
        .apply(lambda x: string_to_ids(x, ADDITIONAL_FILES_LINKS))

    merged_df['test_name_abbreviations'] = merged_df['test_name']\
        .apply(lambda x: safe_str(x))\
        .apply(extract_and_join_abbreviations)

    # üî• –ì–õ–ê–í–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í–ï–¢–ï–†–ò–ù–ê–†–ù–´–• –ê–ë–ë–†–ï–í–ò–ê–¢–£–†
    print("üîß –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É—é –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤ –¥–∞–Ω–Ω—ã–µ...")
    merged_df = enhance_data_with_vet_abbreviations(merged_df)
    
    return merged_df

def enhance_data_with_vet_abbreviations(df):
    
    enhanced_rows = []
    
    for index, row in df.iterrows():
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        test_name = safe_str(row.get('test_name', ''))
        test_code = safe_str(row.get('test_code', ''))
        department = safe_str(row.get('department', ''))
        specialization = safe_str(row.get('specialization', ''))
        
        # üî• –°–û–ó–î–ê–ï–ú –£–õ–£–ß–®–ï–ù–ù–£–Æ –ö–û–õ–û–ù–ö–£ –° –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–ê–ú–ò
        enhanced_text_parts = []
        
        # 1. –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ)
        base_fields = [
            test_name, test_name, 
            department, department,
            specialization,
            safe_str(row.get('type', '')),
            safe_str(row.get('test_name_abbreviations', '')),
            safe_str(row.get('test_name_abbreviations', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('important_information', '')),
            safe_str(row.get('biomaterial_type', '')),
            safe_str(row.get('animal_type', '')),
            safe_str(row.get('container_type', '')),
            safe_str(row.get('storage_temp', ''))
        ]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        enhanced_text_parts.extend([field for field in base_fields if field])
        
        # 2. üî• –î–û–ë–ê–í–õ–Ø–ï–ú –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–´ –ò–ó VET MANAGER
        abbreviation_enhancements = get_abbreviation_enhancements(test_name, test_code, department)
        enhanced_text_parts.extend(abbreviation_enhancements)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å)
        if enhanced_text_parts:
            enhanced_text = ' '.join(enhanced_text_parts)
        else:
            # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            enhanced_text = f"{test_name} {test_code}"
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
        enhanced_row = row.copy()
        enhanced_row['column_for_embeddings'] = enhanced_text
        
        enhanced_rows.append(enhanced_row)
    
    enhanced_df = pd.DataFrame(enhanced_rows)
    
    enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω—ã —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enhanced_df)} —Å—Ç—Ä–æ–∫")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ
    nan_count = enhanced_df['column_for_embeddings'].isna().sum()
    if nan_count > 0:
        print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ")
        # –ó–∞–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
    
    return enhanced_df

def get_abbreviation_enhancements(test_name: str, test_code: str, department: str) -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
    enhancements = []
    
    if not test_name:  # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        return enhancements
        
    test_name_lower = test_name.lower()
    department_lower = department.lower()
    
    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ç–µ—Å—Ç–∞
    for abbr, data in vet_abbr_manager.abbreviations_dict.items():
        full_ru_lower = data['full_ru'].lower()
        
        # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
        if full_ru_lower in test_name_lower:
            enhancements.append(abbr)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è
            for variant in data['variants']:
                if variant.upper() != abbr:
                    enhancements.append(variant)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if data['category']:
                enhancements.append(data['category'])
        
        # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        elif abbr.lower() in test_name_lower:
            enhancements.append(data['full_ru'])
            if data['full_en']:
                enhancements.append(data['full_en'])
    
    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏/—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    if department:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –ø—É—Å—Ç–æ–µ
        for abbr, data in vet_abbr_manager.abbreviations_dict.items():
            if data['category'] and data['category'].lower() in department_lower:
                enhancements.append(abbr)
                enhancements.append(data['full_ru'])
    
    return enhancements

def extract_and_join_abbreviations(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_str = safe_str(text)
    if not text_str:
        return ''
    
    abbreviations = re.findall(r'\(([^()]+)\)', text_str)
    return ' '.join(abbreviations)

def string_to_ids(input_string, mapping_dict):
    if pd.isna(input_string):
        return []
    
    parts = [part.strip() for part in input_string.split('*I*') if part.strip()]
    
    ids = []
    for part in parts:
        if part in mapping_dict:
            ids.append(mapping_dict[part])
    
    return '*I*'.join(ids)


def create_enhanced_dataset():
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏"""
    input_file = "data/processed/data_with_abbreviations_new.xlsx"
    output_file = "data/processed/joined_data.xlsx"
    
    print(f"üîß –°–æ–∑–¥–∞—é —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏...")
    print(f"üì• –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}")
    
    try:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        enhanced_df = join_pcr_data(input_file)
       
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –∫–æ–ª–æ–Ω–∫–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ—Ç NaN
        enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        enhanced_df.to_excel(output_file, index=False)
        
        print(f"üì§ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_file}")
        print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enhanced_df)} –∑–∞–ø–∏—Å–µ–π")
        
        return enhanced_df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        raise

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    result_df = create_enhanced_dataset()
