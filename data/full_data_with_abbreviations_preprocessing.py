import pandas as pd
import re
from bot.handlers.query_processing.vet_abbreviations_expander import vet_abbr_manager


ADDITIONAL_FILES_LINKS = {
    "–ë–ï–®–ï–ù–°–¢–í–û_–ü—Ä–µ–∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ_—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è_–∫_—Ç–µ—Å—Ç—É_AN239RAB_–∏_AN239RABCT": 
    "https://disk.yandex.ru/i/Thne1AF9yvmWlg",
    "–ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–µ_–ø–æ—Å–æ–±–∏–µ_–ø–æ_–≥–∏—Å—Ç–æ–ª–æ–≥–∏–∏": "https://disk.yandex.ru/i/XQi-S8W8lyyFow"
}

GENERAL_FILES_LINkS = {
    "–ì–µ–Ω–µ—Ç–∏–∫–∞": "https://disk.yandex.ru/i/3TTZLfvA6E1mCw",
    "–î–µ—Ä–º–∞—Ç–æ–≥–∏—Å—Ç–æ–ø–∞—Ç–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/ijzv_KTWpQgIYA",
    "–ò–ì–•": "https://disk.yandex.ru/i/qOcf6AX-_HZuzA",
    "–ö–æ—à–∫–∏ –ü–¶–†": "https://disk.yandex.ru/i/Cb5u3CbVLg8hOQ",
    "–ö–†–°": "https://disk.yandex.ru/i/JOSmA7mFIhGTrA",
    "–ö—É—Ä–∏—Ü—ã": "https://disk.yandex.ru/i/EbhWuaW2-R6OmA",
    "–õ–æ—à–∞–¥–∏": "https://disk.yandex.ru/i/zmCgRdeZw9HHqA",
    "–ú–∏–∫—Ä–æ–±–∏–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/DSUU8h0X2o3lJw",
    "–ú–æ—Ä—Å–∫–∏–µ –º–ª–µ–∫–æ–ø–∏—Ç–∞—é—â–∏–µ": "https://disk.yandex.ru/i/jRDFgOiIFIrpVA",
    "–ú–†–°": "https://disk.yandex.ru/i/mVMAnU0-76lACQ",
    "–û–±—â–∏–π": "https://disk.yandex.ru/i/2ZpfPdJx5dOatQ",
    "–ü–∞—Ç–æ–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è": "https://disk.yandex.ru/i/xXm15FsDHwPhpg",
    "–°–≤–∏–Ω—å–∏": "https://disk.yandex.ru/i/R-IofXZ0qpwLSA",
    "–°–æ–±–∞–∫–∏ –ü–¶–†": "https://disk.yandex.ru/i/ZXrs4sNjduf4OQ"
}


def safe_str(value, default=""):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É, —É–±–∏—Ä–∞—è nan"""
    if pd.isna(value) or value is None or str(value).lower() in ['nan', 'none', 'null', '']:
        return ''
    return str(value).strip()

def clean_embedding_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –æ—Ç —à—É–º–Ω—ã—Ö —Å–ª–æ–≤"""
    if not text or pd.isna(text):
        return ""
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()
    
    # –£–¥–∞–ª—è–µ–º –æ–±—â–∏–µ —à—É–º–Ω—ã–µ —Å–ª–æ–≤–∞ (–≤—Å–µ —Ñ–æ—Ä–º—ã –∏ –ø–∞–¥–µ–∂–∏)
    noise_patterns = [
        r'–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏[–µ–π—è—é]\w*',  # –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏ —Ç.–¥.
        r'–∞–Ω–∞–ª–∏–∑[–∞—É–æ–º]?\w*',      # –∞–Ω–∞–ª–∏–∑, –∞–Ω–∞–ª–∏–∑–∞, –∞–Ω–∞–ª–∏–∑—É, –∞–Ω–∞–ª–∏–∑–æ–º
        r'–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏[–µ–π—è—é]\w*',   # –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
        r'—Ç–µ—Å—Ç[–∞—É–æ–º]?\w*',        # —Ç–µ—Å—Ç, —Ç–µ—Å—Ç–∞, —Ç–µ—Å—Ç—É, —Ç–µ—Å—Ç–æ–º
        r'–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫[–∞–∏—É–µ]\w*',   # –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏, –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É
        r'–∏–∑—É—á–µ–Ω–∏[–µ–π—è—é]\w*',      # –∏–∑—É—á–µ–Ω–∏–µ, –∏–∑—É—á–µ–Ω–∏—è, –∏–∑—É—á–µ–Ω–∏—é
        r'–ø—Ä–æ–≤–µ–¥–µ–Ω–∏[–µ–π—è—é]\w*',    # –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ, –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è, –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—é
        r'–∏–∑–º–µ—Ä–µ–Ω–∏[–µ–π—è—é]\w*',     # –∏–∑–º–µ—Ä–µ–Ω–∏–µ, –∏–∑–º–µ—Ä–µ–Ω–∏—è, –∏–∑–º–µ—Ä–µ–Ω–∏—é
        r'–æ—Ü–µ–Ω–∫[–∞–∏—É–µ]\w*',        # –æ—Ü–µ–Ω–∫–∞, –æ—Ü–µ–Ω–∫–∏, –æ—Ü–µ–Ω–∫—É
        r'–ø—Ä–æ—Ñ–∏–ª[–µ–π—è—é]\w*',       # –ø—Ä–æ—Ñ–∏–ª—å, –ø—Ä–æ—Ñ–∏–ª—è, –ø—Ä–æ—Ñ–∏–ª—é
        r'–∫–æ–º–ø–ª–µ–∫—Å–Ω\w*',          # –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π, –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è, –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ
        r'–æ–±—â[–∏–µ–π—è—é]\w*',         # –æ–±—â–∏–π, –æ–±—â–∞—è, –æ–±—â–µ–µ, –æ–±—â–∏–µ
        r'—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω\w*',          # —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π, —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è
        r'—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω\w*',          # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è
        r'–º–∞–ª\w*',                # –º–∞–ª—ã–π, –º–∞–ª–∞—è, –º–∞–ª–æ–µ
        r'–±–æ–ª—å—à\w*',              # –±–æ–ª—å—à–æ–π, –±–æ–ª—å—à–∞—è, –±–æ–ª—å—à–æ–µ
        r'–ø–µ—Ä–≤–∏—á–Ω\w*',            # –ø–µ—Ä–≤–∏—á–Ω—ã–π, –ø–µ—Ä–≤–∏—á–Ω–∞—è
        r'–∫–æ–Ω—Ç—Ä–æ–ª[–µ–π—è—é]\w*',      # –∫–æ–Ω—Ç—Ä–æ–ª—å, –∫–æ–Ω—Ç—Ä–æ–ª—è, –∫–æ–Ω—Ç—Ä–æ–ª—é
        r'–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥[–∞—É]?\w*',    # –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        r'–ø—Ä–æ–±[–∞—ã—É–µ]\w*',         # –ø—Ä–æ–±–∞, –ø—Ä–æ–±—ã, –ø—Ä–æ–±—É
        r'—É—Ä–æ–≤–Ω[–µ–π—è—é]\w*',        # —É—Ä–æ–≤–µ–Ω—å, —É—Ä–æ–≤–Ω—è, —É—Ä–æ–≤–Ω—é
        r'—Å–æ–¥–µ—Ä–∂–∞–Ω–∏[–µ–π—è—é]\w*',    # —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        r'–∫–æ–ª–∏—á–µ—Å—Ç–≤[–∞—É–æ–º]?\w*',   # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        r'–Ω–∞–ª–∏—á–∏[–µ–π—è—é]\w*',       # –Ω–∞–ª–∏—á–∏–µ, –Ω–∞–ª–∏—á–∏—è
        r'–ø–æ–∫–∞–∑–∞—Ç–µ–ª[–µ–π—è—é]\w*',    # –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å, –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è
        r'–ø–∞—Ä–∞–º–µ—Ç—Ä[–∞–æ–≤]?\w*',     # –ø–∞—Ä–∞–º–µ—Ç—Ä, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        r'–º–µ—Ç–æ–¥[–∞—É–æ–º]?\w*',       # –º–µ—Ç–æ–¥, –º–µ—Ç–æ–¥–∞, –º–µ—Ç–æ–¥—É
        r'—Å–ø–æ—Å–æ–±[–∞—É–æ–º]?\w*',      # —Å–ø–æ—Å–æ–±, —Å–ø–æ—Å–æ–±–∞, —Å–ø–æ—Å–æ–±—É
    ]
    
    for pattern in noise_patterns:
        text = re.sub(pattern, '', text)
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –æ—á–∏—â–∞–µ–º
    text = re.sub(r'\s+', ' ', text).strip()
    
    # –£–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ –∫–æ—Ä–æ—á–µ 3 —Å–∏–º–≤–æ–ª–æ–≤ (–∫—Ä–æ–º–µ –∫–æ–¥–æ–≤ —Ç–∏–ø–∞ –ü–¶–†, –ò–§–ê –∏ —Ç.–¥.)
    words = text.split()
    filtered_words = []
    
    special_short_words = {'–ø—Ü—Ä', '–∏—Ñ–∞', '—ç–¥—Ç–∞', '–¥–Ω–∫', '—Ä–Ω–∫', '–∞—Ç', '–∞–≥', 'igg', 'igm', 'ca', 'cd', 'cv', '–Ω–µ'}
    
    for word in words:
        if len(word) >= 1 or word in special_short_words:
            filtered_words.append(word)
    
    return ' '.join(filtered_words)

def join_pcr_data(main_file_path):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
    
    # –ß–∏—Ç–∞–µ–º –æ–±–∞ –ª–∏—Å—Ç–∞ –∏–∑ Excel —Ñ–∞–π–ª–∞
    main_df = pd.read_excel(main_file_path, sheet_name='–û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞')
    pcr_df = pd.read_excel(main_file_path, sheet_name='–°–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π –ü–¶–†')
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±—É–∫–≤–µ–Ω–Ω–æ–π —á–∞—Å—Ç–∏ –∏–∑ –∫–æ–¥–∞ —Ç–µ—Å—Ç–∞
    def extract_letters(code):
        code_str = safe_str(code)
        if not code_str:
            return None
        match = re.search(r'(\d+)([A-Za-z–ê-–Ø–∞-—è]+)$', code_str)
        if match:
            return match.group(2).strip()
        return None
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –±—É–∫–≤–µ–Ω–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∫–æ–¥–æ–≤ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    main_df['code_letters'] = main_df['test_code'].apply(extract_letters)
    main_df['code_letters'] = main_df['code_letters'].apply(lambda x: safe_str(x).upper())
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç–æ–ª–±–µ—Ü —Å –±—É–∫–≤–µ–Ω–Ω—ã–º–∏ —á–∞—Å—Ç—è–º–∏ –∫–æ–¥–æ–≤ –¥–ª—è —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –ü–¶–†
    pcr_df.rename(columns={'–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞': 'code_letters'}, inplace=True)
    pcr_df['code_letters'] = pcr_df['code_letters'].apply(lambda x: safe_str(x).strip())
    pcr_df['–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'] = pcr_df['–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞'].apply(lambda x: ' '.join(safe_str(x).split("/")))
                                                                                     
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –ø–æ –±—É–∫–≤–µ–Ω–Ω—ã–º —á–∞—Å—Ç—è–º –∫–æ–¥–æ–≤
    merged_df = pd.merge(main_df, pcr_df, 
                        left_on='code_letters', 
                        right_on='code_letters', 
                        how='left')

    merged_df.rename(columns={'–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞': 'encoded'}, inplace=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏
    merged_df['form_link'] = merged_df['form_name']\
        .apply(lambda x: safe_str(x))\
        .apply(lambda x: string_to_ids(x, GENERAL_FILES_LINkS))

    merged_df['additional_information_link'] = merged_df['additional_information_name']\
        .apply(lambda x: safe_str(x))\
        .apply(lambda x: string_to_ids(x, ADDITIONAL_FILES_LINKS))

    merged_df['test_name_abbreviations'] = merged_df['test_name']\
        .apply(lambda x: safe_str(x))\
        .apply(extract_and_join_abbreviations)

    # üî• –ì–õ–ê–í–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í–ï–¢–ï–†–ò–ù–ê–†–ù–´–• –ê–ë–ë–†–ï–í–ò–ê–¢–£–† –í –î–ê–ù–ù–´–ï
    print("üîß –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É—é –≤–µ—Ç–µ—Ä–∏–Ω–∞—Ä–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤ –¥–∞–Ω–Ω—ã–µ...")
    merged_df = enhance_data_with_vet_abbreviations(merged_df)
    
    return merged_df

def enhance_data_with_vet_abbreviations(df):
    
    enhanced_rows = []
    
    for index, row in df.iterrows():
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
        test_name = safe_str(row.get('test_name', ''))
        test_code = safe_str(row.get('test_code', ''))
        department = safe_str(row.get('department', ''))
        specialization = safe_str(row.get('specialization', ''))
        
        # üî• –°–û–ó–î–ê–ï–ú –£–õ–£–ß–®–ï–ù–ù–£–Æ –ö–û–õ–û–ù–ö–£ –° –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–ê–ú–ò
        enhanced_text_parts = []
        
        # 1. –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ)
        base_fields = [
            test_name, test_name, test_name, test_name,
            department, department,
            specialization,
            safe_str(row.get('type', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('code_letters', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('encoded', '')),
            safe_str(row.get('biomaterial_type', '')),
            safe_str(row.get('biomaterial_type', '')),
            safe_str(row.get('biomaterial_type', '')),
            safe_str(row.get('test_name_abbreviations', '')),
            safe_str(row.get('test_name_abbreviations', '')),
            safe_str(row.get('test_name_abbreviations', '')),
            # safe_str(row.get('important_information', '')),
            safe_str(row.get('animal_type', '')),
            safe_str(row.get('animal_type', '')),
            safe_str(row.get('animal_type', '')),
            safe_str(row.get('container_type', '')),
            # safe_str(row.get('storage_temp', ''))
        ]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        enhanced_text_parts.extend([field for field in base_fields if field])
        
        # 2. üî• –î–û–ë–ê–í–õ–Ø–ï–ú –ê–ë–ë–†–ï–í–ò–ê–¢–£–†–´ –ò–ó VET MANAGER
        abbreviation_enhancements = get_abbreviation_enhancements(test_name, test_code, department)
        enhanced_text_parts.extend(abbreviation_enhancements)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å —á—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å)
        if enhanced_text_parts:
            enhanced_text = ' '.join(enhanced_text_parts)
        else:
            # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ª—è –ø—É—Å—Ç—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            enhanced_text = f"{test_name} {test_code}"
        
        # üî• –í–ê–ñ–ù–û–ï –£–õ–£–ß–®–ï–ù–ò–ï: –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê –î–õ–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
        cleaned_enhanced_text = clean_embedding_text(enhanced_text)
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é —Å—Ç—Ä–æ–∫—É
        enhanced_row = row.copy()
        enhanced_row['column_for_embeddings'] = cleaned_enhanced_text.lower()
        enhanced_row['column_for_embeddings_raw'] = enhanced_text  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–∏–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        
        enhanced_rows.append(enhanced_row)
    
    enhanced_df = pd.DataFrame(enhanced_rows)
    
    enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω—ã —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enhanced_df)} —Å—Ç—Ä–æ–∫")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ
    nan_count = enhanced_df['column_for_embeddings'].isna().sum()
    if nan_count > 0:
        print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ")
        # –ó–∞–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN –Ω–∞ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
    
    return enhanced_df

def get_abbreviation_enhancements(test_name: str, test_code: str, department: str) -> list:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä"""
    enhancements = []
    
    if not test_name:  # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—É—Å—Ç–æ–µ, –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        return enhancements
        
    test_name_lower = test_name.lower()
    department_lower = department.lower()
    
    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ç–µ—Å—Ç–∞
    for abbr, data in vet_abbr_manager.abbreviations_dict.items():
        full_ru_lower = data['full_ru'].lower()
        
        # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ - –¥–æ–±–∞–≤–ª—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
        if full_ru_lower in test_name_lower:
            enhancements.append(abbr)
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è
            for variant in data['variants']:
                if variant.upper() != abbr:
                    enhancements.append(variant)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            if data['category']:
                enhancements.append(data['category'])
        
        # –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        elif abbr.lower() in test_name_lower:
            enhancements.append(data['full_ru'])
            if data['full_en']:
                enhancements.append(data['full_en'])
    
    # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ –æ—Ç–¥–µ–ª–µ–Ω–∏–∏/—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    if department:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–¥–µ–ª–µ–Ω–∏–µ –Ω–µ –ø—É—Å—Ç–æ–µ
        for abbr, data in vet_abbr_manager.abbreviations_dict.items():
            if data['category'] and data['category'].lower() in department_lower:
                enhancements.append(abbr)
                enhancements.append(data['full_ru'])
    
    return enhancements

def extract_and_join_abbreviations(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_str = safe_str(text)
    if not text_str:
        return ''
    
    abbreviations = re.findall(r'\(([^()]+)\)', text_str)
    return ' '.join(abbreviations)

def string_to_ids(input_string, mapping_dict):
    if pd.isna(input_string):
        return []
    
    parts = [part.strip() for part in input_string.split('*I*') if part.strip()]
    
    ids = []
    for part in parts:
        if part in mapping_dict:
            ids.append(mapping_dict[part])
    
    return '*I*'.join(ids)


def create_enhanced_dataset():
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏"""
    input_file = "data/processed/data_with_abbreviations_new.xlsx"
    output_file = "data/processed/joined_data.xlsx"
    
    print(f"üîß –°–æ–∑–¥–∞—é —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞–º–∏...")
    print(f"üì• –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {input_file}")
    
    try:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        enhanced_df = join_pcr_data(input_file)
       
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤ –∫–æ–ª–æ–Ω–∫–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ—Ç NaN
        enhanced_df['column_for_embeddings'] = enhanced_df['column_for_embeddings'].fillna('')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        enhanced_df.to_excel(output_file, index=False)
        
        print(f"üì§ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_file}")
        print(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(enhanced_df)} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏
        print("\nüìä –ü—Ä–∏–º–µ—Ä—ã –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:")
        sample_data = enhanced_df[['column_for_embeddings_raw', 'column_for_embeddings']].head(3)
        for _, row in sample_data.iterrows():
            print(f"–î–æ: {row['column_for_embeddings_raw'][:100]}...")
            print(f"–ü–æ—Å–ª–µ: {row['column_for_embeddings']}")
            print("---")
        
        return enhanced_df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        raise

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
if __name__ == "__main__":
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    result_df = create_enhanced_dataset()